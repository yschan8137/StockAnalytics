{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yschan8137/StockAnalytics/blob/master/TRivots_U_L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5LGAJ35eXrH",
        "outputId": "328b2dd1-eb82-474a-dc3d-94f43c186cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#預載套件\n"
      ],
      "metadata": {
        "id": "Q8_dkoCbmsgZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V1IxboH5jFT",
        "outputId": "7a882e00-bd1a-4133-adc7-3c202afc5293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.6.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.290 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 2.34.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.2.1 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.3.2 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.3.2 which is incompatible.\n",
            "earthengine-api 0.1.290 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 2.34.0 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#!pip install dash-bootstrap-components jupyter-dash plotly -q > log.txt\n",
        "!pip install shioaji --upgrade > log.txt\n",
        "#!pip install --upgrade shioaji > log.txt\n",
        "#!pip install plotly --upgrade > log.txt\n",
        "!pip install --upgrade pandas-gbq 'google-cloud-bigquery[bqstorage,pandas]' > log.txt\n",
        "#!pip install talib-binary > log.txt\n",
        "#!pip install yfinance > log.txt\n",
        "!pip install --upgrade google-api-python-client > log.txt\n",
        "!pip install --upgrade google-cloud-storage > log.txt\n",
        "!pip install tqdm > log.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Or8kZ9QqurL"
      },
      "outputs": [],
      "source": [
        "import shioaji\n",
        "#import dash\n",
        "#from jupyter_dash import JupyterDash\n",
        "#import dash_core_components as dcc\n",
        "#import dash_html_components as html\n",
        "#from dash.dependencies import Output, Input, State\n",
        "#import plotly.graph_objs as go\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import pandas_gbq as pd_gbq\n",
        "import os\n",
        "#import sqlite3\n",
        "#import yfinance as yf\n",
        "%load_ext google.cloud.bigquery \n",
        "%load_ext google.colab.data_table"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bigquery認證"
      ],
      "metadata": {
        "id": "cxHv10Y-mzPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JGd15olRAe_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632ebe42-254c-4376-e3e9-ee9512c7a884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import googleapiclient\n",
        "from googleapiclient import discovery\n",
        "\n",
        "project_id = 'project4bigquery'\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/content/drive/MyDrive/project4bigquery_credential.json'\n",
        "    #'/content/project4bigquery_credential.json'\n",
        "        )\n",
        "\n",
        "scoped_credentials = credentials.with_scopes(\n",
        "    ['https://www.googleapis.com/auth/cloud-platform'])\n",
        "\n",
        "client = bigquery.Client(project=project_id, credentials= credentials, )\n",
        "compute = discovery.build('compute', 'v1', credentials=credentials)\n",
        "%load_ext google.colab.data_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4apoRN9bN6EG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f2b09e-37d8-494d-de7d-6eb8ab5dfefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import googleapiclient\n",
        "from googleapiclient import discovery\n",
        "\n",
        "project_id = 'crypto-trading-20210130'\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/content/drive/MyDrive/crypto-trading-20210130-098eb822fa12.json')\n",
        "\n",
        "scoped_credentials = credentials.with_scopes(\n",
        "    ['https://www.googleapis.com/auth/cloud-platform'])\n",
        "\n",
        "client = bigquery.Client(project=project_id, credentials= credentials)\n",
        "compute = googleapiclient.discovery.build('compute', 'v1', credentials=credentials)\n",
        "%load_ext google.colab.data_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MALFmvD0apCB"
      },
      "source": [
        "#建立Instances\n",
        "https://dev.to/zohebabai/boost-your-colab-notebooks-with-gcp-and-aws-instance-within-a-few-minutes-47ma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AarmPQF2nIcu"
      },
      "outputs": [],
      "source": [
        "!gcloud init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fOpPU21aoCX"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkmqhZbJ4Gj2",
        "outputId": "891f0f13-b063-4441-df37-05e4b76e8f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: GCE_PROJECT_NAME=project4bigquery\n",
            "env: ACCOUNT=yschan1101026@gmail.com\n",
            "env: ZONE=asia-east1-b\n",
            "env: VM_NAME=colab-1-vm\n",
            "env: TPU_IP_RANGE=10.240.1.0/29\n",
            "env: GCS_DATA_PATH=gs://cloud-tpu-data/mnist/\n",
            "env: GCS_CKPT_PATH=gs://cloud-tpu-checkpoint/mnist/\n"
          ]
        }
      ],
      "source": [
        "%env GCE_PROJECT_NAME project4bigquery\n",
        "%env ACCOUNT yschan1101026@gmail.com\n",
        "%env ZONE asia-east1-b\n",
        "%env VM_NAME colab-1-vm\n",
        "%env TPU_IP_RANGE 10.240.1.0/29\n",
        "%env GCS_DATA_PATH gs://cloud-tpu-data/mnist/\n",
        "%env GCS_CKPT_PATH gs://cloud-tpu-checkpoint/mnist/\n",
        "    \n",
        "# Automatically get bucket name from GCS paths\n",
        "import os\n",
        "os.environ['GCS_DATA_BUCKET'] = os.environ['GCS_DATA_PATH'][5:].split('/')[0]\n",
        "os.environ['GCS_CKPT_BUCKET'] = os.environ['GCS_CKPT_PATH'][5:].split('/')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4bbbpgCHC7e",
        "outputId": "89b5ee90-8b50-49b4-d5d8-dd74c9a0fac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/account].\n",
            "Updated property [compute/zone].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set account $ACCOUNT\n",
        "!gcloud services enable storage.googleapis.com --project $GCE_PROJECT_NAME\n",
        "!gcloud config set compute/zone $ZONE \n",
        "#!gcloud config set core/project $GCE_PROJECT_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3h4tV2hZNfC",
        "outputId": "badad21f-d734-4c86-cab8-3f47ca370009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No existing items\n"
          ]
        }
      ],
      "source": [
        "zone= %env ZONE\n",
        "def list_instances(compute= compute, project= project_id, zone= zone):\n",
        "    result = compute.instances().list(project=project, zone=zone).execute()\n",
        "    return result['items'] if 'items' in result else print('No existing items')\n",
        "list_instances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riy1sOd34Gj7",
        "outputId": "da297ddb-57b8-4aec-96a0-9421fc8ce534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created [https://www.googleapis.com/compute/beta/projects/project4bigquery/zones/asia-east1-b/instances/colab-1-vm].\n",
            "NAME        ZONE          MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS\n",
            "colab-1-vm  asia-east1-b  e2-standard-4               10.140.0.30  35.229.218.126  RUNNING\n"
          ]
        }
      ],
      "source": [
        "#!gcloud alpha compute tpus create $TPU_NAME --range=$TPU_IP_RANGE --accelerator-type= --version=nightly --zone=$TPU_ZONE\n",
        "!gcloud beta compute instances create $VM_NAME \\\n",
        "    --machine-type=e2-standard-4 \\\n",
        "    --zone=$ZONE \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vfkTX8ZhJc2"
      },
      "outputs": [],
      "source": [
        "!gcloud compute ssh --zone $ZONE $VM_NAME -- -L 8888:localhost:8888"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjDCLSEfSW0Y",
        "outputId": "99f940b5-eff1-4c59-efc0-dac372af52a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following instances will be deleted. Any attached disks configured to be \n",
            "auto-deleted will be deleted unless they are attached to any other instances or \n",
            "the `--keep-disks` flag is given and specifies them for keeping. Deleting a disk\n",
            " is irreversible and any data on the disk will be lost.\n",
            " - [colab-1-vm] in [asia-east1-b]\n",
            "\n",
            "Do you want to continue (Y/n)?  \n",
            "Deleted [https://www.googleapis.com/compute/beta/projects/project4bigquery/zones/asia-east1-b/instances/colab-1-vm].\n"
          ]
        }
      ],
      "source": [
        "!yes | gcloud beta compute instances delete $VM_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFMwVwLR2tk"
      },
      "source": [
        "# Shioaji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss73-doZZgpv",
        "outputId": "55a0c69b-a880-4c24-e585-f5758aa904a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Code: 0 | Event Code: 0 | Info: host '203.66.91.161:80', IP 203.66.91.161:80 (host 1 of 1) (host connection attempt 1 of 1) (total connection attempt 1 of 1) | Event: Session up\n",
            "<SecurityType.Index: 'IND'> fetch done.\n",
            "<SecurityType.Future: 'FUT'> fetch done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FutureAccount(person_id='E124281388', broker_id='F002000', account_id='1564279', username='詹詠翔'),\n",
              " Account(account_type=<AccountType.H: 'H'>, person_id='E124281388', broker_id='9A95', account_id='0184746', username='詹詠翔'),\n",
              " StockAccount(person_id='E124281388', broker_id='9A95', account_id='0631362', username='詹詠翔\\u3000\\u3000')]"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ],
      "source": [
        "# 匯入 Shioaji 套件\n",
        "import shioaji as sj\n",
        "\n",
        "# 建立 Shioaji api 物件\n",
        "api = sj.Shioaji()\n",
        "\n",
        "# 登入帳號\n",
        "api.login(\n",
        "     person_id=\"E124281388\",\n",
        "     passwd=\"lz5014225\",\n",
        "     contracts_cb=lambda security_type: print(f\"{repr(security_type)} fetch done.\")\n",
        "     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmHUEp3fcMKa"
      },
      "source": [
        "# 列出股票及期貨清單"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlKBfu_VWGXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942941b4-354b-4281-caf6-79d06872b814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['close_5min', 'close_day', 'high_5min', 'high_day', 'low_5min', 'low_day', 'open_5min', 'open_day', 'volume_5min', 'volume_day'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 檢查BigQuery現有資料表\n",
        "dataset_ref = client.dataset('kbar', project= project_id)\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "tables =client.get_table('kbar.close_5min')\n",
        "tables = list(client.list_tables(dataset))\n",
        "table = [table.table_id for table in tables]\n",
        "print(table, '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzw4KnRRyfGi",
        "outputId": "07410239-24ba-4a3e-c33e-ddf3e075ecf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "股票數量:1745\n"
          ]
        }
      ],
      "source": [
        "from shioaji.constant import Exchange\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "contracts = api.Contracts\n",
        "stock = contracts.Stocks\n",
        "future = contracts.Futures\n",
        "\n",
        "stock_list = []\n",
        "for s in stock:\n",
        "  for f in s:\n",
        "    stock_list.append({**f})\n",
        "df_stock = pd.DataFrame(stock_list).set_index('code').dropna()\n",
        "#df_stock.to_gbq(destination_table= 'my_database.stock_list', project_id= project_id, if_exists='replace', location= 'asia-east1') #存在台灣的資料庫\n",
        "\n",
        "future_list = []\n",
        "for s in future:\n",
        "  for f in s:\n",
        "    future_list.append({**f})\n",
        "df_future = pd.DataFrame(future_list).set_index('code').dropna()\n",
        "#df_future.to_gbq(destination_table= 'my_database.future_list', project_id= project_id, if_exists='replace', location= 'asia-east1')\n",
        "\n",
        "## 存入股票代號及名稱\n",
        "LAST_TRADE_DATE = max(df_stock.update_date)\n",
        "list_stock = []\n",
        "for exchange in stock:\n",
        "    for stocks in exchange:\n",
        "        if stocks.exchange in (Exchange.TSE, Exchange.OTC):\n",
        "          if stocks.category == '00' or stocks.category == '':\n",
        "                continue\n",
        "          elif stocks.update_date != LAST_TRADE_DATE: #剔除已下市股票\n",
        "                continue\n",
        "          elif re.search('[A-Z]', stocks.code) is None:\n",
        "            list_stock.append([stocks.code, stocks.name, stocks.update_date, stocks.day_trade])\n",
        "\n",
        "df = pd.DataFrame(list_stock, columns= ['Code', 'Name', 'update_date', 'day_trade'])\n",
        "df.index = df.index + 1\n",
        "df.update_date = pd.to_datetime(df.update_date)\n",
        "#df.to_gbq(f'{dataset}.list_stock', project_id= project_id, if_exists= 'replace', table_schema= [{'name': 'Code', 'type':'STRING'},{'name': 'Name', 'type':'STRING'}, {'name': 'update_date', 'type':'TIMESTAMP'}, {'name': 'day_trade', 'type':'STRING'}] )\n",
        "print(f'股票數量:{len(df)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d38sjmNgXx8m"
      },
      "source": [
        "#查詢現有資料日期及欄位"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9WPE_EAip6q"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "from talib.abstract import *\n",
        "import numpy as np\n",
        "\n",
        "now = str(dt.now().date())\n",
        "start=\"2018-12-07\" \n",
        "end= now\n",
        "## Get ticks all day\n",
        "import pandas as pd\n",
        "import time\n",
        "stock = api.Contracts.Stocks\n",
        "\n",
        "stocks_table = {'code':[], 'name':[], 'update_date':[], 'symbol':[]}\n",
        "for exchange in stock:\n",
        "    for stocks in exchange:\n",
        "        if stocks.exchange in (Exchange.TSE, Exchange.OTC):\n",
        "          if stocks.category == '00' or stocks.category == '':\n",
        "                continue\n",
        "#          elif stocks.update_date != max(stocks.update_date): #剔除已下市股票\n",
        "#                continue\n",
        "          elif re.search('[A-Z]', stocks.code) is None:\n",
        "            stocks_table['code'].append(stocks.code)\n",
        "            stocks_table['name'].append(stocks.name)\n",
        "            stocks_table['update_date'].append(stocks.update_date)\n",
        "            stocks_table['symbol'].append(stocks.symbol)\n",
        "\n",
        "stocks_table = pd.DataFrame(stocks_table)\n",
        "stocks_table.set_index('symbol', inplace= True)\n",
        "LAST_TRADE_DATE = pd.to_datetime(max(stocks_table.update_date)).strftime('%Y-%m-%d')\n",
        "\n",
        "def kbar(*args):\n",
        "  for input in args:\n",
        "#    for stocks in {**stock.TSE, **stock.OTC}.values():\n",
        "      if input in set(stocks_table.code):\n",
        "        stockcode = str(input)\n",
        "        kbars = api.kbars(api.Contracts.Stocks[f'{stockcode}'], start=start)\n",
        "        df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts), ).drop('ts', axis= 1)\n",
        "      elif input in set(stocks_table.name):\n",
        "        stockcode = stocks_table[(stocks_table.name == input)].code.values[0]\n",
        "        kbars = api.kbars(api.Contracts.Stocks[f'{stockcode}'], start=start)\n",
        "        df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "  return df\n",
        "n= 0\n",
        "#for code in stocks_table.code[0:4]:\n",
        "#  print([*stocks_table[stocks_table.code == code].name][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tACwDzr7UHSN"
      },
      "outputs": [],
      "source": [
        "df_0050 = pd.DataFrame({**api.kbars(stock['0050'], start= '2018-12-07')}).set_index('ts')\n",
        "df_0050.index = pd.to_datetime(df_0050.index)\n",
        "list_date = df_0050.resample('D').agg(\n",
        "    {\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last'}   \n",
        ").dropna(how='all')\n",
        "list_date = pd.to_datetime(list_date.index.date)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 確認現有欄位是否需更新外，亦可在無資料時重新抓取資料"
      ],
      "metadata": {
        "id": "ZgJvxvUIu-8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KDJ7cbgswWK0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import tqdm.notebook as tq\n",
        "import sys\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from shioaji.constant import Exchange\n",
        "import re\n",
        "import shioaji as sj\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import googleapiclient\n",
        "from googleapiclient import discovery\n",
        "\n",
        "api = sj.Shioaji()\n",
        "\n",
        "class gbq_trading:\n",
        "  def __init__(self, id, passwd):\n",
        "    self.start = '2018-12-07'\n",
        "    self.now = str(dt.now().date())\n",
        "    if not api.stock_account:\n",
        "      api.login(\n",
        "         person_id= id,\n",
        "         passwd= passwd,\n",
        "         contracts_cb=lambda security_type: print(f\"{repr(security_type)} fetch done.\")\n",
        "         )\n",
        "      self.stock = api.Contracts.Stocks\n",
        "    else:\n",
        "      self.stock = api.Contracts.Stocks   \n",
        "  \n",
        "  def stocks_table(self):\n",
        "    stocks_table = {'code':[], 'name':[], 'update_date':[], 'symbol':[]}\n",
        "    for exchange in self.stock:\n",
        "        for stocks in exchange:\n",
        "            if stocks.exchange in (Exchange.TSE, Exchange.OTC):\n",
        "              if stocks.category == '00' or stocks.category == '':\n",
        "                continue\n",
        "              elif re.search('[A-Z]', stocks.code) is None:\n",
        "                stocks_table['code'].append(stocks.code)\n",
        "                stocks_table['name'].append(stocks.name)\n",
        "                stocks_table['update_date'].append(stocks.update_date)\n",
        "                stocks_table['symbol'].append(stocks.symbol)\n",
        "    stocks_table = pd.DataFrame(stocks_table)\n",
        "    stocks_table.set_index('symbol', inplace= True)\n",
        "    return stocks_table[stocks_table.update_date == max(stocks_table.update_date)]     \n",
        "\n",
        "  def list_date(self): \n",
        "    # all kbar date data \n",
        "    df_0050 = pd.DataFrame({**api.kbars(self.stock['0050'], start= self.start)}).set_index('ts')\n",
        "    df_0050.index = pd.to_datetime(df_0050.index)\n",
        "    list_date = df_0050.resample('D').agg({\n",
        "                'Open': 'first', \n",
        "                'High': 'max', \n",
        "                'Low': 'min', \n",
        "                'Close': 'last'}   \n",
        "                ).replace(0, np.NAN).dropna(how='all')\n",
        "    list_date = pd.to_datetime(list_date.index.date)\n",
        "    return list_date\n",
        "\n",
        "  def kbar_shioaji(self, args, start= '2018-12-07', end= dt.now().strftime('%Y-%m-%d')): \n",
        "    # download kbar data from shioaji \n",
        "    # instead of using *args in argumnets to allow list as input\n",
        "    sotck = self.stock\n",
        "    stocks_table = self.stocks_table()\n",
        "    df_kbars = []\n",
        "    keys = []\n",
        "    for input in tq.tqdm(args, desc= 'Downloading', position= 0):\n",
        "      if input in [stocks.code for stocks in {**stock.TSE, **stock.OTC}.values()]:\n",
        "        keys.append(input)\n",
        "        kbars = api.kbars(stock[f'{input}'], start= start, end= end)\n",
        "        df_kbars.append(pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1).resample(rule='1Min', closed = 'right').agg({\n",
        "                     'Open': 'first', \n",
        "                     'High': 'max',            \n",
        "                     'Low': 'min',\n",
        "                     'Close': 'last',\n",
        "                     'Volume': 'sum'}).dropna(axis= 0, how= 'all'))\n",
        "      elif input in [stocks.name for stocks in {**stock.TSE, **stock.OTC}.values()]:\n",
        "        stockcode = stocks_table[stocks_table.name == input].code[0]\n",
        "        keys.append(stockcode)    \n",
        "        kbars = api.kbars(stock[f'{stockcode}'], start= start, end= end)\n",
        "        df_kbars.append(pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1).resample(rule='1Min', closed = 'right').agg({\n",
        "                     'Open': 'first', \n",
        "                     'High': 'max',            \n",
        "                     'Low': 'min',\n",
        "                     'Close': 'last',\n",
        "                     'Volume': 'sum'}).dropna(axis= 0, how= 'all'))\n",
        "      elif input not in [*stocks_table.code] + [*stocks_table.name]:\n",
        "        raise KeyError(f'請確認{input}是否正確')\n",
        "      dfs = pd.concat(df_kbars, axis= 1, keys= keys)    \n",
        "    return dfs\n",
        "\n",
        "  def kbar_gbq(self, args):\n",
        "    # query pre-stored kbar data from Google bigquery \n",
        "    stocks_table = self.stocks_table()\n",
        "    df_kbars = []\n",
        "    columns = {}\n",
        "    for input in args:\n",
        "      if input in stocks_table.code.values:\n",
        "        dfs = client.query('''\n",
        "                    SELECT\n",
        "                    ts,\n",
        "                    {}\n",
        "                    FROM\n",
        "                    `{}.{}.{}`\n",
        "                    ORDER BY ts ASC \n",
        "                    '''.format(stocks_table[stocks_table.code.values == input].index[0], project_id, dataset, table_id)).to_dataframe().set_index('ts')\n",
        "        df_kbars.append(dfs)\n",
        "        columns[dfs.columns[0]] = input\n",
        "      elif input in stocks_table.name.values:\n",
        "        dfs = client.query('''\n",
        "                    SELECT\n",
        "                    ts,\n",
        "                    {}\n",
        "                    FROM\n",
        "                    `{}.{}.{}`\n",
        "                    ORDER BY ts ASC \n",
        "                    '''.format(stocks_table[stocks_table.name.values == input].index[0], project_id, dataset, table_id)).to_dataframe().set_index('ts')\n",
        "        df_kbars.append(dfs)\n",
        "        columns[dfs.columns[0]] = stocks_table[stocks_table.name == input].code.values[0]\n",
        "\n",
        "    dfs = pd.concat(df_kbars, axis= 1).rename(columns= columns)\n",
        "    return dfs\n",
        "\n",
        "  def update_to_gbq(self, project_id, dataset, credentials_path):\n",
        "    self.stocks_table = self.stocks_table()\n",
        "    update_date = self.stocks_table.update_date\n",
        "    list_date = self.list_date()\n",
        "    df_list = {}\n",
        "    df_to_add_list = {}\n",
        "    column_name = {}\n",
        "\n",
        "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
        "    scoped_credentials = credentials.with_scopes(['https://www.googleapis.com/auth/cloud-platform'])\n",
        "    client = bigquery.Client(project= project_id, credentials= credentials)\n",
        "\n",
        "    dataset_ref = client.dataset('kbar', project= project_id)\n",
        "    datasets = client.get_dataset(dataset_ref)\n",
        "    tables =client.get_table('kbar.close_5min')\n",
        "    tables = list(client.list_tables(datasets))\n",
        "    table_list = [table.table_id for table in tables]\n",
        "    \n",
        "    for table_id in table_list:\n",
        "      update = 0\n",
        "      new = 0\n",
        "      #To get full table information and allow appending collumns at first\n",
        "      #查詢最新更新日期\n",
        "      update_time= {} #避免保留上一輪loop的table_list資料\n",
        "      last_update= {}\n",
        "      init_update= {}\n",
        "      for id in table_list[table_list.index(table_id):]: \n",
        "        try:\n",
        "          update_time[id] = pd.to_datetime(client.query('''\n",
        "                                  SELECT\n",
        "                                  ts\n",
        "                                  FROM\n",
        "                                  `{}.{}.{}`\n",
        "                                  ORDER BY ts ASC\n",
        "                                  '''.format(project_id, dataset, id)).to_dataframe().ts.values).tz_localize(None)\n",
        "          last_update[id] = update_time[id].max()\n",
        "          init_update[id] = update_time[id].min()\n",
        "        except:\n",
        "          print(f'Oops! 須確認{id}的更新日期')\n",
        "          update_time[id] = []\n",
        "          last_update[id] = pd.to_datetime('2018-12-06') #如果沒有資料，設定成可下載期間初始日的前一日，在計算回推日期時，初始點(point_str)才是正確的\n",
        "          init_update[id] = pd.to_datetime('2018-12-06')\n",
        "  \n",
        "      #檢查現有欄位\n",
        "      column_name[f'{table_id}'] =  client.query('''\n",
        "                              SELECT\n",
        "                              * \n",
        "                              FROM\n",
        "                              `{}.{}`.INFORMATION_SCHEMA.COLUMNS\n",
        "                              WHERE table_name=\"{}\"'''.format(project_id, dataset, table_id)).to_dataframe().column_name.values.tolist()[1:] #省略ts\n",
        "\n",
        "      #允許新增欄位\n",
        "      job_config = bigquery.QueryJobConfig(\n",
        "                  destination= f\"{project_id}.{dataset}.{table_id}\",\n",
        "                  schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
        "                  write_disposition= bigquery.WriteDisposition.WRITE_APPEND,\n",
        "                  )\n",
        "\n",
        "      #如果沒有全數table_list的資料，則逐批重新抓取\n",
        "      if len(update_time[f'{table_id}'])== 0:   \n",
        "        n = 0\n",
        "        intv = 200\n",
        "        for intv in tq.tqdm([list_date[i:i+intv] for i in range(0, len(list_date), intv)], desc= '分批重新下載', position= 0):\n",
        "\n",
        "          date_begin= intv[0]  \n",
        "          date_end = intv[-1]\n",
        "          sys.stdout.write('\\r' +f'下載{date_begin}到{date_end}的資料')\n",
        "          n += 1\n",
        "          new = 0   #下一個time loop 從頭開始計算次數\n",
        "\n",
        "          for table in table_list:\n",
        "            df_list[table]= pd.DataFrame([])\n",
        "\n",
        "          for code in tq.tqdm(self.stocks_table.code, desc= '重新抓取資料', position= 1):\n",
        "            kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= date_begin.strftime('%Y-%m-%d'), end= date_end.strftime('%Y-%m-%d'))\n",
        "            df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "            df.index.rename('ts', inplace= True)\n",
        "            df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "                'Open': 'first', \n",
        "                'High': 'max', \n",
        "                'Low': 'min', \n",
        "                'Close': 'last',\n",
        "                'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "            df_day = df.resample(rule='D').agg({\n",
        "                'Open': 'first', \n",
        "                'High': 'max', \n",
        "                'Low': 'min', \n",
        "                'Close': 'last',\n",
        "                'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "            df_5min.sort_index(inplace= True)\n",
        "            df_day.sort_index(inplace= True)\n",
        "            #5min\n",
        "            #close\n",
        "            df_list['close_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Close       \n",
        "            #open\n",
        "            df_list['open_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Open\n",
        "            #high\n",
        "            df_list['high_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.High\n",
        "            #low\n",
        "            df_list['low_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Low\n",
        "            #volume\n",
        "            df_list['volume_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Volume\n",
        "\n",
        "            #day\n",
        "            #close\n",
        "            df_list['close_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Close\n",
        "            #open\n",
        "            df_list['open_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Open\n",
        "            #high\n",
        "            df_list['high_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.High\n",
        "            #low\n",
        "            df_list['low_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Low\n",
        "            #volume\n",
        "            df_list['volume_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Volume\n",
        "            new += 1\n",
        "            sys.stdout.write('\\r' + f'{table_id}第{n}輪已併入{new}項，還剩下{len(self.stocks_table.index) - new + 1}項')\n",
        "            sys.stdout.flush()\n",
        "\n",
        "      ##存入資料\n",
        "          for id in [id for id in table_list if len(update_time[id]) != len(list_date)]:\n",
        "            df_list[id].reset_index().to_gbq(f'{dataset}.{id}', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "            sys.stdout.write('\\r' + f'第{n}輪重新下載{id}')\n",
        "            sys.stdout.flush()\n",
        "      #若有現有欄位，則須確認資料是否有缺漏\n",
        "      elif column_name[f'{table_id}']:\n",
        "\n",
        "        existing_data = [dt for dt in list_date if dt in update_time[f'{table_id}'].date] #現有資料日期\n",
        "        miss_data = [dt for dt in list_date if dt not in update_time[f'{table_id}'].date] #缺少資料的日期\n",
        "\n",
        "        if len(existing_data) == len(list_date) and last_update[f'{table_id}'].date() == list_date[-1]: #若更新日期為最新且資料長度與資料來源一致，則可以判斷是最新資料\n",
        "          print('\\r' + f'{table_id}已是最新資料')\n",
        "          continue\n",
        "\n",
        "        elif min(miss_data) > last_update[f'{table_id}']: #如果缺失資料的最早日期晚於最後更新日期，則只要更新最後更新日期之後的資料\n",
        "\n",
        "          point_str = -sum(list_date > last_update[f'{table_id}'])\n",
        "          sys.stdout.write('\\r' + f\"{table_id}須更新{list_date[point_str].date()}後的資料\")\n",
        "\n",
        "          for table in table_list:\n",
        "            df_list[table] = pd.DataFrame([])\n",
        "            df_to_add_list[table] = pd.DataFrame([])\n",
        "\n",
        "          for code in tq.tqdm(self.stocks_table.loc[[symbol for symbol in self.stocks_table.index if symbol in column_name[f'{table_id}']]].code, desc= '更新現有欄位資料', position= 1):\n",
        "            kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= min(miss_data).strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "            df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "            df.index.rename('ts', inplace= True)\n",
        "            df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "                'Open': 'first', \n",
        "                'High': 'max', \n",
        "                'Low': 'min', \n",
        "                'Close': 'last',\n",
        "                'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "            df_day = df.resample(rule='D').agg({\n",
        "                'Open': 'first', \n",
        "                'High': 'max', \n",
        "                'Low': 'min', \n",
        "                'Close': 'last',\n",
        "                'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "            df_5min.sort_index(inplace= True)\n",
        "            df_day.sort_index(inplace= True)\n",
        "\n",
        "            df_list['close_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Close\n",
        "            df_list['open_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Open\n",
        "            df_list['high_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.High\n",
        "            df_list['low_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Low    \n",
        "            df_list['volume_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Volume\n",
        "\n",
        "            df_list['close_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Close\n",
        "            df_list['open_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Open\n",
        "            df_list['high_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.High\n",
        "            df_list['low_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Low\n",
        "            df_list['volume_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Volume\n",
        "            update += 1\n",
        "            sys.stdout.write('\\r' + f'{table_id}還剩下{len(column_name[table_id]) - update}待更新，並須插入{len([symbols for symbols in stocks_table.index if symbols not in column_name[table_id]])}項')\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        elif min(miss_data) < last_update[f'{table_id}']: #若缺失的最早資料早於最後更新日期，代表在最後更新日期前的資料有缺少\n",
        "\n",
        "          for table in table_list:\n",
        "            df_list[table] = pd.DataFrame([])\n",
        "            df_to_add_list[table] = pd.DataFrame([])      \n",
        "\n",
        "          for date in tq.tqdm([miss_data[i:i+1] for i in range(0, len(miss_data), 1)], desc= '補齊缺失資料', position= 1):\n",
        "            for code in tq.tqdm(stocks_table.loc[[symbol for symbol in self.stocks_table.index if symbol in column_name[f'{table_id}']]].code, desc= '更新現有欄位資料', position= 1):\n",
        "              kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= date[0].strftime('%Y-%m-%d'), end= date[0].strftime('%Y-%m-%d'))\n",
        "              df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "              df.index.rename('ts', inplace= True)\n",
        "              df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "                  'Open': 'first', \n",
        "                  'High': 'max', \n",
        "                  'Low': 'min', \n",
        "                  'Close': 'last',\n",
        "                  'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "              df_day = df.resample(rule='D').agg({\n",
        "                  'Open': 'first', \n",
        "                  'High': 'max', \n",
        "                  'Low': 'min', \n",
        "                  'Close': 'last',\n",
        "                  'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "              df_5min.sort_index(inplace= True)\n",
        "              df_day.sort_index(inplace= True)\n",
        "              #close_5min['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "              #close_5min.index.rename('ts', inplace= True)\n",
        "              df_list['close_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Close\n",
        "              df_list['open_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Open\n",
        "              df_list['high_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.High\n",
        "              df_list['low_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Low    \n",
        "              df_list['volume_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min.Volume\n",
        "\n",
        "              df_list['close_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Close\n",
        "              df_list['open_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Open\n",
        "              df_list['high_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.High\n",
        "              df_list['low_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Low\n",
        "              df_list['volume_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day.Volume\n",
        "              update += 1\n",
        "              sys.stdout.write('\\r' + f'{table_id}還剩下{len(column_name[table_id]) - update}待更新，並須插入{len(stocks_table.index) - len(column_name[table_id])}項')\n",
        "              sys.stdout.flush()\n",
        "\n",
        "        for code in tq.tqdm(self.stocks_table.loc[[symbol for symbol in self.stocks_table.index if symbol not in column_name[f'{table_id}']]].code, desc= '新增欄位', position= 1):\n",
        "          kbars_to_add = api.kbars(api.Contracts.Stocks[f'{code}'], start= list_date[0].strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "          df_to_add = pd.DataFrame({**kbars_to_add}, index= pd.to_datetime(kbars_to_add.ts)).drop('ts', axis= 1)\n",
        "          df_to_add.index.rename('ts', inplace= True)\n",
        "          df_5min_to_add = df_to_add.resample(rule='5Min', closed='right').agg({\n",
        "              'Open': 'first', \n",
        "              'High': 'max', \n",
        "              'Low': 'min', \n",
        "              'Close': 'last',\n",
        "              'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "          df_day_to_add = df_to_add.resample(rule='D').agg({\n",
        "              'Open': 'first', \n",
        "              'High': 'max', \n",
        "              'Low': 'min', \n",
        "              'Close': 'last',\n",
        "              'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "          df_5min_to_add.sort_index(inplace= True)\n",
        "          df_day_to_add.sort_index(inplace= True)\n",
        "          df_to_add_list['close_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min_to_add.Close\n",
        "          df_to_add_list['open_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min_to_add.Open\n",
        "          df_to_add_list['high_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min_to_add.High\n",
        "          df_to_add_list['low_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min_to_add.Low    \n",
        "          df_to_add_list['volume_5min']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_5min_to_add.Volume\n",
        "          df_to_add_list['close_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day_to_add.Close\n",
        "          df_to_add_list['open_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day_to_add.Open\n",
        "          df_to_add_list['high_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day_to_add.High\n",
        "          df_to_add_list['low_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day_to_add.Low\n",
        "          df_to_add_list['volume_day']['{}'.format(self.stocks_table[self.stocks_table.code == code].index[0])] = df_day_to_add.Volume\n",
        "          new += 1\n",
        "          sys.stdout.write('\\r' + f'{table_id}已插入{new}項，還剩下{len(self.stocks_table.index) - len(column_name[table_id]) - new}項')\n",
        "          sys.stdout.flush()\n",
        "      #存入資料\n",
        "      for id in [id for id in last_update.keys() if last_update[id].date() != list_date[-1]]:\n",
        "        if id:\n",
        "          df_list[id].reset_index().to_gbq(f'{dataset}.{id}', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "\n",
        "      if not [symbol for symbol in stocks_table.index if symbol not in column_name]:\n",
        "        table = client.get_table(f'{dataset}.{table_id}')  # Make an API request.\n",
        "\n",
        "        original_schema = table.schema\n",
        "        new_schema = original_schema[:]  # Creates a copy of the schema.\n",
        "        for new_columns in [*df_to_add[f'{table_id}'].columns]:\n",
        "          new_schema.append(bigquery.SchemaField(f'{new_columns}', 'FLOAT'))\n",
        "        table.schema = new_schema\n",
        "        table = client.update_table(table, [\"schema\"])  # Make an API request.\n",
        "\n",
        "        if len(table.schema) == len(original_schema) + 1 == len(new_schema):\n",
        "          print(\"A new column has been added.\")\n",
        "        else:\n",
        "          print(\"The column has not been added.\")\n",
        "          # Start the query, passing in the extra configuration.\n",
        "        client.load_table_from_dataframe(df_to_add[f'{table_id}'], f'{dataset}.{table_id}')\n",
        "    print('Done') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TEST"
      ],
      "metadata": {
        "id": "DATmcmDCk3IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#table_list = ['close_5min', 'open_5min', 'high_5min', 'low_5min', 'volume_5min']#, 'close_day', 'open_day', 'high_day', 'low_day', 'volume_day']\n",
        "#update_time = {}\n",
        "#dataset= 'kbar'\n",
        "#for table_id in table_list:\n",
        "#  update_time[f'{table_id}'] = pd.to_datetime(client.query('''\n",
        "#                          SELECT\n",
        "#                          ts\n",
        "#                          FROM\n",
        "#                          `{}.{}.{}`\n",
        "#                          ORDER BY ts ASC\n",
        "#                          '''.format(project_id, dataset, table_id)).to_dataframe().ts.values)#.tz_localize(None)\n",
        "##update_time['close_5min'][-sum(update_time['close_5min'] < list_date[-1])].date()\n",
        "#miss = [dt for dt in list_date if dt in update_time['close_5min'].date]#.to_period('D').groupby(update_time['close_5min'].to_period('D')).keys()]\n",
        "#intv = 2\n",
        "#for date in [miss[i:i+intv] for i in range(0, len(miss), intv)]:\n",
        "#  print(date[0].strftime('%Y-%m-%d'))\n",
        "#D = update_time['close_5min'].to_period('D')\n",
        "#len(D.groupby(D).keys())\n",
        "#min(miss) > last_update['close_5min']\n",
        "#update_time['close_5min'].date\n",
        "#point_str = -(sum(update_time['close_5min'] < list_date[-1]))\n",
        "#update_time['close_5min'][point_str].strftime('%Y-%m-%d')\n",
        "#init_update['close_5min']\n",
        "#min(miss).strftime('%Y-%m-%d')\n",
        "#update_time[f'{table_id}'].date\n",
        "#table_id= 'close_5min'\n",
        "#df = client.query(\"\"\"\n",
        "#        SELECT\n",
        "#        ts,\n",
        "#        OTC8034, \n",
        "#        OTC5487, \n",
        "#        OTC5386, \n",
        "#        OTC2745,\n",
        "#        FROM\n",
        "#        `{}.{}.{}`\n",
        "#        ORDER BY\n",
        "#        ts\n",
        "#        \"\"\".format(project_id, dataset, table_id)).to_dataframe().set_index('ts')\n",
        "#df = df['2018-12-07':'2019-06-30'].iloc[0:-1:48].append(df['2019-07-01': '2019-12-31'].iloc[0:-1:72])\n",
        "#update_date = pd.to_datetime(df.index.values)\n",
        "#miss_data = [dt for dt in list_date if dt not in update_date.date]\n",
        "#\n",
        "#for date in [miss_data[i:i+2] for i in range(0, len(miss_data), 2)]:\n",
        "#list_date.strftime('%Y-%m-%d').to_list().index('2021-12-17')  \n",
        "#miss_data\n",
        "#for date in [miss_data[i:i+2] for i in range(0, len(miss_data), 2)if (list_date.to_list().index(miss_data[i]) - list_date.to_list().index(miss_data[i -1]) != 1) | (list_date.to_list().index(miss_data[i]) - list_date.to_list().index(miss_data[i -1]) != -1) ]:\n",
        "#  print(date)"
      ],
      "metadata": {
        "id": "xYt6OCigAdK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 關閉虛擬主機"
      ],
      "metadata": {
        "id": "tEfGp15nvkSi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDbhMdhDK4vJ"
      },
      "outputs": [],
      "source": [
        "!yes | gcloud beta compute instances delete $VM_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#以前版本"
      ],
      "metadata": {
        "id": "_yQ4CO7omSqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###從Shioaji抓資料及查詢Bigquery資料"
      ],
      "metadata": {
        "id": "vH_2FqqLRcf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock = api.Contracts.Stocks\n",
        "def kbar_gbq(args):\n",
        "  df_kbars = []\n",
        "  columns = {}\n",
        "  for input in args:\n",
        "    if input in stocks_table.code.values:\n",
        "      dfs = client.query('''\n",
        "                  SELECT\n",
        "                  ts,\n",
        "                  {}\n",
        "                  FROM\n",
        "                  `{}.{}.{}`\n",
        "                  ORDER BY ts ASC \n",
        "                  '''.format(stocks_table[stocks_table.code.values == input].index[0], project_id, dataset, table_id)).to_dataframe().set_index('ts')\n",
        "      df_kbars.append(dfs)\n",
        "      columns[dfs.columns[0]] = input\n",
        "    elif input in stocks_table.name.values:\n",
        "      dfs = client.query('''\n",
        "                  SELECT\n",
        "                  ts,\n",
        "                  {}\n",
        "                  FROM\n",
        "                  `{}.{}.{}`\n",
        "                  ORDER BY ts ASC \n",
        "                  '''.format(stocks_table[stocks_table.name.values == input].index[0], project_id, dataset, table_id)).to_dataframe().set_index('ts')\n",
        "      df_kbars.append(dfs)\n",
        "      columns[dfs.columns[0]] = stocks_table[stocks_table.name == '台積電'].code.values[0]\n",
        "\n",
        "  dfs = pd.concat(df_kbars, axis= 1).rename(columns= columns)\n",
        "  return dfs\n",
        "#dfs.rename(columns= {dfs.columns[0]: keys})\n",
        "\n",
        "def kbar_shioaji(args, start= '2018-12-07', end= dt.now().strftime('%Y-%m-%d')): \n",
        "  # download kbar data from shioaji \n",
        "  # instead of using *args in argumnets to allow list as input\n",
        "  df_kbars = []\n",
        "  keys = []\n",
        "  for input in tq.tqdm(args, desc= 'Downloading', position= 0):\n",
        "    if input in [stocks.code for stocks in {**stock.TSE, **stock.OTC}.values()]:\n",
        "      keys.append(input)\n",
        "      kbars = api.kbars(stock[f'{input}'], start= start, end= end)\n",
        "      df_kbars.append(pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1).resample(rule='1Min', closed = 'right').agg({\n",
        "                   'Open': 'first', \n",
        "                   'High': 'max',            \n",
        "                   'Low': 'min',\n",
        "                   'Close': 'last',\n",
        "                   'Volume': 'sum'}).dropna(axis= 0, how= 'all'))\n",
        "    elif input in [stocks.name for stocks in {**stock.TSE, **stock.OTC}.values()]:\n",
        "      stockcode = stocks_table[stocks_table.name == input].code[0]\n",
        "      keys.append(stockcode)    \n",
        "      kbars = api.kbars(stock[f'{stockcode}'], start= start, end= end)\n",
        "      df_kbars.append(pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1).resample(rule='1Min', closed = 'right').agg({\n",
        "                   'Open': 'first', \n",
        "                   'High': 'max',            \n",
        "                   'Low': 'min',\n",
        "                   'Close': 'last',\n",
        "                   'Volume': 'sum'}).dropna(axis= 0, how= 'all'))\n",
        "    elif input not in [*stocks_table.code] + [*stocks_table.name]:\n",
        "      raise KeyError(f'請確認{input}是否正確')\n",
        "    dfs = pd.concat(df_kbars, axis= 1, keys= keys)    \n",
        "  return dfs"
      ],
      "metadata": {
        "id": "q77rlbhf2FvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ver.4"
      ],
      "metadata": {
        "id": "upn58UaSRKi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bfSWSBQB9f_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import tqdm.notebook as tq\n",
        "import sys\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from shioaji.constant import Exchange\n",
        "import re\n",
        "import shioaji as sj\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import googleapiclient\n",
        "from googleapiclient import discovery\n",
        "\n",
        "start=\"2018-12-07\" \n",
        "end = str(dt.now().date())\n",
        "dataset = 'kbar'\n",
        "\n",
        "stocks_table = {'code':[], 'name':[], 'update_date':[], 'symbol':[]}\n",
        "for exchange in stock:\n",
        "    for stocks in exchange:\n",
        "        if stocks.exchange in (Exchange.TSE, Exchange.OTC):\n",
        "          if stocks.category == '00' or stocks.category == '':\n",
        "                continue\n",
        "          elif re.search('[A-Z]', stocks.code) is None:\n",
        "            stocks_table['code'].append(stocks.code)\n",
        "            stocks_table['name'].append(stocks.name)\n",
        "            stocks_table['update_date'].append(stocks.update_date)\n",
        "            stocks_table['symbol'].append(stocks.symbol)\n",
        "stocks_table = pd.DataFrame(stocks_table)\n",
        "stocks_table.set_index('symbol', inplace= True)\n",
        "LAST_TRADE_DATE = max(stocks_table.update_date)\n",
        "stocks_table = stocks_table[stocks_table.update_date == LAST_TRADE_DATE]\n",
        "\n",
        "def kbar(*args):\n",
        "  for input in args:\n",
        "    for stocks in {**stock.TSE, **stock.OTC}.values():\n",
        "      if input in set(stocks_table.code):\n",
        "        stockcode = str(input)\n",
        "        kbars = api.kbars(api.Contracts.Stocks[f'{stockcode}'], start=start)\n",
        "        df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts), ).drop('ts', axis= 1)\n",
        "      elif input in set(stocks_table.name):\n",
        "        stockcode = stocks_table[(stocks_table.name == input)].code.values[0]\n",
        "        kbars = api.kbars(api.Contracts.Stocks[f'{stockcode}'], start=start)\n",
        "        df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "  return df\n",
        "\n",
        "\n",
        "#透過0050取得完整資料序列\n",
        "df_0050 = pd.DataFrame({**api.kbars(stock['0050'], start= '2018-12-07')}).set_index('ts')\n",
        "df_0050.index = pd.to_datetime(df_0050.index)\n",
        "list_date = df_0050.resample('D').agg({\n",
        "            'Open': 'first', \n",
        "            'High': 'max', \n",
        "            'Low': 'min', \n",
        "            'Close': 'last'}   \n",
        "            ).dropna(how='all')\n",
        "list_date = pd.to_datetime(list_date.index.date)\n",
        "\n",
        "#取得現有資料表\n",
        "dataset_ref = client.dataset('kbar', project= project_id)\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "tables =client.get_table('kbar.close_5min')\n",
        "tables = list(client.list_tables(dataset))\n",
        "table_list = [table.table_id for table in tables]\n",
        "\n",
        "#更新資料\n",
        "dataset= 'kbar'\n",
        "#table_list = ['close_5min', 'open_5min', 'high_5min', 'low_5min', 'volume_5min', 'close_day', 'open_day', 'high_day', 'low_day', 'volume_day'] \n",
        "update_date = stocks_table.update_date\n",
        "df_list = {}\n",
        "df_to_add_list = {}\n",
        "column_name = {}\n",
        "\n",
        "for table_id in table_list:\n",
        "  update = 0\n",
        "  new = 0\n",
        "  #To get full table information and allow appending collumns at first\n",
        "  #查詢最新更新日期\n",
        "  update_time= {} #避免保留上一輪loop的table_list資料\n",
        "  last_update= {}\n",
        "  init_update= {}\n",
        "  for id in table_list[table_list.index(table_id):]: \n",
        "    try:\n",
        "      update_time[id] = pd.to_datetime(client.query('''\n",
        "                              SELECT\n",
        "                              ts\n",
        "                              FROM\n",
        "                              `{}.{}.{}`\n",
        "                              ORDER BY ts ASC\n",
        "                              '''.format(project_id, dataset, id)).to_dataframe().ts.values).tz_localize(None)\n",
        "      last_update[id] = update_time[id].max()\n",
        "      init_update[id] = update_time[id].min()\n",
        "    except:\n",
        "      print(f'Oops! 須確認{id}的更新日期')\n",
        "      update_time[id] = []\n",
        "      last_update[id] = pd.to_datetime('2018-12-06') #如果沒有資料，設定成可下載期間初始日的前一日，在計算回推日期時，初始點(point_str)才是正確的\n",
        "      init_update[id] = pd.to_datetime('2018-12-06')\n",
        "  \n",
        "  #檢查現有欄位\n",
        "  column_name[f'{table_id}'] =  client.query('''\n",
        "                          SELECT\n",
        "                          * \n",
        "                          FROM\n",
        "                          `{}.{}`.INFORMATION_SCHEMA.COLUMNS\n",
        "                          WHERE table_name=\"{}\"'''.format(project_id, dataset, table_id)).to_dataframe().column_name.values.tolist()[1:] #省略ts\n",
        "\n",
        "  #允許新增欄位\n",
        "  job_config = bigquery.QueryJobConfig(\n",
        "              destination= f\"{project_id}.{dataset}.{table_id}\",\n",
        "              schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
        "              write_disposition= bigquery.WriteDisposition.WRITE_APPEND,\n",
        "              )\n",
        "\n",
        "  #如果沒有全數table_list的資料，則逐批重新抓取\n",
        "  if len(update_time[f'{table_id}'])== 0:   \n",
        "    n = 0\n",
        "    intv = 200\n",
        "    for intv in tq.tqdm([list_date[i:i+intv] for i in range(0, len(list_date), intv)], desc= '分批重新下載', position= 0):\n",
        "      \n",
        "      date_begin= intv[0]  \n",
        "      date_end = intv[-1]\n",
        "      sys.stdout.write('\\r' +f'下載{date_begin}到{date_end}的資料')\n",
        "      n += 1\n",
        "      new = 0   #下一個time loop 從頭開始計算次數\n",
        "\n",
        "      for table in table_list:\n",
        "        df_list[table]= pd.DataFrame([])\n",
        "\n",
        "      for code in tq.tqdm(stocks_table.code, desc= '重新抓取資料', position= 1):\n",
        "        kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= date_begin.strftime('%Y-%m-%d'), end= date_end.strftime('%Y-%m-%d'))\n",
        "        df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "        df.index.rename('ts', inplace= True)\n",
        "        df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "            'Open': 'first', \n",
        "            'High': 'max', \n",
        "            'Low': 'min', \n",
        "            'Close': 'last',\n",
        "            'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "        df_day = df.resample(rule='D').agg({\n",
        "            'Open': 'first', \n",
        "            'High': 'max', \n",
        "            'Low': 'min', \n",
        "            'Close': 'last',\n",
        "            'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "        df_5min.sort_index(inplace= True)\n",
        "        df_day.sort_index(inplace= True)\n",
        "        #5min\n",
        "        #close\n",
        "        df_list['close_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close       \n",
        "        #open\n",
        "        df_list['open_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Open\n",
        "        #high\n",
        "        df_list['high_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.High\n",
        "        #low\n",
        "        df_list['low_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Low\n",
        "        #volume\n",
        "        df_list['volume_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Volume\n",
        "\n",
        "        #day\n",
        "        #close\n",
        "        df_list['close_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Close\n",
        "        #open\n",
        "        df_list['open_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Open\n",
        "        #high\n",
        "        df_list['high_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.High\n",
        "        #low\n",
        "        df_list['low_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Low\n",
        "        #volume\n",
        "        df_list['volume_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Volume\n",
        "        new += 1\n",
        "        sys.stdout.write('\\r' + f'{table_id}第{n}輪已併入{new}項，還剩下{len(stocks_table.index) - new + 1}項')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "  ##存入資料\n",
        "      for id in [id for id in table_list if len(update_time[id]) != len(list_date)]:\n",
        "        df_list[id].reset_index().to_gbq(f'{dataset}.{id}', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "        sys.stdout.write('\\r' + f'第{n}輪重新下載{id}')\n",
        "        sys.stdout.flush()\n",
        "      #df_list['close_5min'].reset_index().to_gbq(f'{dataset}.close_5min', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['open_5min'].reset_index().to_gbq(f'{dataset}.open_5min', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['high_5min'].reset_index().to_gbq(f'{dataset}.high_5min', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['low_5min'].reset_index().to_gbq(f'{dataset}.low_5min', project_id= project_id, if_exists='replace')\n",
        "      #df_list['volume_5min'].reset_index().to_gbq(f'{dataset}.volume_5min', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "\n",
        "      #df_list['close_day'].reset_index().to_gbq(f'{dataset}.close_day', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['open_day'].reset_index().to_gbq(f'{dataset}.open_day', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['high_day'].reset_index().to_gbq(f'{dataset}.high_day', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['low_day'].reset_index().to_gbq(f'{dataset}.low_day', project_id= project_id, if_exists='replace', credentials= credentials)\n",
        "      #df_list['volume_day'].reset_index().to_gbq(f'{dataset}.volume_day', project_id= project_id, if_exists='replace', credentials= credentials)   \n",
        "  \n",
        "  #若有現有欄位，則須確認資料是否有缺漏\n",
        "  elif column_name[f'{table_id}']:\n",
        "    \n",
        "    existing_data = [dt for dt in list_date if dt in update_time[f'{table_id}'].date] #現有資料日期\n",
        "    miss_data = [dt for dt in list_date if dt not in update_time[f'{table_id}'].date] #缺少資料的日期\n",
        "\n",
        "    if len(existing_data) == len(list_date) and last_update[f'{table_id}'].date() == list_date[-1]: #若更新日期為最新且資料長度與資料來源一致，則可以判斷是最新資料\n",
        "      print(f'{table_id}已是最新資料')\n",
        "      continue\n",
        "\n",
        "    elif min(miss_data) > last_update[f'{table_id}']: #如果缺失資料的最早日期晚於最後更新日期，則只要更新最後更新日期之後的資料\n",
        "\n",
        "      point_str = -sum(list_date > last_update[f'{table_id}'])\n",
        "      sys.stdout.write('\\r' + f\"{table_id}須更新{list_date[point_str].date()}後的資料\")\n",
        "\n",
        "      for table in table_list:\n",
        "        df_list[table] = pd.DataFrame([])\n",
        "        df_to_add_list[table] = pd.DataFrame([])\n",
        "\n",
        "      for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol in column_name[f'{table_id}']]].code, desc= '更新現有欄位資料', position= 1):\n",
        "        kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= min(miss_data).strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "        df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "        df.index.rename('ts', inplace= True)\n",
        "        df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "            'Open': 'first', \n",
        "            'High': 'max', \n",
        "            'Low': 'min', \n",
        "            'Close': 'last',\n",
        "            'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "        df_day = df.resample(rule='D').agg({\n",
        "            'Open': 'first', \n",
        "            'High': 'max', \n",
        "            'Low': 'min', \n",
        "            'Close': 'last',\n",
        "            'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "        df_5min.sort_index(inplace= True)\n",
        "        df_day.sort_index(inplace= True)\n",
        "        #close_5min['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "        #close_5min.index.rename('ts', inplace= True)\n",
        "        df_list['close_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "        df_list['open_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Open\n",
        "        df_list['high_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.High\n",
        "        df_list['low_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Low    \n",
        "        df_list['volume_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Volume\n",
        "\n",
        "        df_list['close_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Close\n",
        "        df_list['open_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Open\n",
        "        df_list['high_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.High\n",
        "        df_list['low_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Low\n",
        "        df_list['volume_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Volume\n",
        "        update += 1\n",
        "        sys.stdout.write('\\r' + f'{table_id}還剩下{len(column_name[table_id]) - update}待更新，並須插入{len([symbols for symbols in stocks_table.index if symbols not in column_name[table_id]])}項')\n",
        "        sys.stdout.flush()\n",
        "    \n",
        "    elif min(miss_data) < last_update[f'{table_id}']: #若缺失的最早資料早於最後更新日期，代表在最後更新日期前的資料有缺少\n",
        "\n",
        "      for table in table_list:\n",
        "        df_list[table] = pd.DataFrame([])\n",
        "        df_to_add_list[table] = pd.DataFrame([])      \n",
        "      \n",
        "      for date in tq.tqdm([miss_data[i:i+1] for i in range(0, len(miss_data), 1)], desc= '補齊缺失資料', position= 1):\n",
        "        for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol in column_name[f'{table_id}']]].code, desc= '更新現有欄位資料', position= 1):\n",
        "          kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= date[0].strftime('%Y-%m-%d'), end= date[0].strftime('%Y-%m-%d'))\n",
        "          df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "          df.index.rename('ts', inplace= True)\n",
        "          df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "              'Open': 'first', \n",
        "              'High': 'max', \n",
        "              'Low': 'min', \n",
        "              'Close': 'last',\n",
        "              'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "          df_day = df.resample(rule='D').agg({\n",
        "              'Open': 'first', \n",
        "              'High': 'max', \n",
        "              'Low': 'min', \n",
        "              'Close': 'last',\n",
        "              'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "          df_5min.sort_index(inplace= True)\n",
        "          df_day.sort_index(inplace= True)\n",
        "          #close_5min['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "          #close_5min.index.rename('ts', inplace= True)\n",
        "          df_list['close_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "          df_list['open_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Open\n",
        "          df_list['high_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.High\n",
        "          df_list['low_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Low    \n",
        "          df_list['volume_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Volume\n",
        "\n",
        "          df_list['close_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Close\n",
        "          df_list['open_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Open\n",
        "          df_list['high_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.High\n",
        "          df_list['low_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Low\n",
        "          df_list['volume_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day.Volume\n",
        "          update += 1\n",
        "          sys.stdout.write('\\r' + f'{table_id}還剩下{len(column_name[table_id]) - update}待更新，並須插入{len(stocks_table.index) - len(column_name[table_id])}項')\n",
        "          sys.stdout.flush()\n",
        "\n",
        "    for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol not in column_name[f'{table_id}']]].code, desc= '新增欄位', position= 1):\n",
        "      kbars_to_add = api.kbars(api.Contracts.Stocks[f'{code}'], start= list_date[0].strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "      df_to_add = pd.DataFrame({**kbars_to_add}, index= pd.to_datetime(kbars_to_add.ts)).drop('ts', axis= 1)\n",
        "      df_to_add.index.rename('ts', inplace= True)\n",
        "      df_5min_to_add = df_to_add.resample(rule='5Min', closed='right').agg({\n",
        "          'Open': 'first', \n",
        "          'High': 'max', \n",
        "          'Low': 'min', \n",
        "          'Close': 'last',\n",
        "          'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "      df_day_to_add = df_to_add.resample(rule='D').agg({\n",
        "          'Open': 'first', \n",
        "          'High': 'max', \n",
        "          'Low': 'min', \n",
        "          'Close': 'last',\n",
        "          'Volume': 'sum'}).replace(0, np.NaN).dropna(axis= 0, how= 'all')\n",
        "      df_5min_to_add.sort_index(inplace= True)\n",
        "      df_day_to_add.sort_index(inplace= True)\n",
        "      df_to_add_list['close_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.Close\n",
        "      df_to_add_list['open_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.Open\n",
        "      df_to_add_list['high_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.High\n",
        "      df_to_add_list['low_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.Low    \n",
        "      df_to_add_list['volume_5min']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.Volume\n",
        "      df_to_add_list['close_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day_to_add.Close\n",
        "      df_to_add_list['open_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day_to_add.Open\n",
        "      df_to_add_list['high_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day_to_add.High\n",
        "      df_to_add_list['low_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day_to_add.Low\n",
        "      df_to_add_list['volume_day']['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_day_to_add.Volume\n",
        "      new += 1\n",
        "      sys.stdout.write('\\r' + f'{table_id}已插入{new}項，還剩下{len(stocks_table.index) - len(column_name[table_id]) - new}項')\n",
        "      sys.stdout.flush()\n",
        "  #存入資料\n",
        "  for id in [id for id in last_update.keys() if last_update[id].date() != list_date[-1]]:\n",
        "    if id:\n",
        "      client.load_table_from_dataframe(df_list[f'{table_id}'], f'{dataset}.{table_id}')\n",
        "      #df_list[id].reset_index().to_gbq(f'{dataset}.{id}', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['close_5min'].reset_index().to_gbq(f'{dataset}.close_5min', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['open_5min'].reset_index().to_gbq(f'{dataset}.open_5min', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['high_5min'].reset_index().to_gbq(f'{dataset}.high_5min', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['low_5min'].reset_index().to_gbq(f'{dataset}.low_5min', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['volume_5min'].reset_index().to_gbq(f'{dataset}.volume_5min', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "\n",
        "  #df_list['close_day'].reset_index().to_gbq(f'{dataset}.close_day', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['open_day'].reset_index().to_gbq(f'{dataset}.open_day', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['high_day'].reset_index().to_gbq(f'{dataset}.high_day', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['low_day'].reset_index().to_gbq(f'{dataset}.low_day', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "  #df_list['volume_day'].reset_index().to_gbq(f'{dataset}.volume_day', project_id= project_id, if_exists='append', credentials= credentials)\n",
        "\n",
        "  if not [symbol for symbol in stocks_table.index if symbol not in column_name]:\n",
        "    table = client.get_table(f'{dataset}.{table_id}')  # Make an API request.\n",
        "\n",
        "    original_schema = table.schema\n",
        "    new_schema = original_schema[:]  # Creates a copy of the schema.\n",
        "    for new_columns in [*df_to_add[f'{table_id}'].columns]:\n",
        "      new_schema.append(bigquery.SchemaField(f'{new_columns}', 'FLOAT'))\n",
        "    table.schema = new_schema\n",
        "    table = client.update_table(table, [\"schema\"])  # Make an API request.\n",
        "\n",
        "    if len(table.schema) == len(original_schema) + 1 == len(new_schema):\n",
        "      print(\"A new column has been added.\")\n",
        "    else:\n",
        "      print(\"The column has not been added.\")\n",
        "      # Start the query, passing in the extra configuration.\n",
        "    client.load_table_from_dataframe(df_to_add[f'{table_id}'], f'{dataset}.{table_id}')\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ver.3(只確認現有欄位是否須更新)"
      ],
      "metadata": {
        "id": "_4YYzfMjkvFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3uiP17Jxdpt"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "n= 0\n",
        "new = 0\n",
        "dataset= 'kbar'\n",
        "table_id = ['close_5min', 'open_5min', 'high_5min', 'low_5min', 'volume_5min', 'close_day', 'open_day', 'high_day', 'low_day', 'volume_day'] \n",
        "#查詢現有欄位\n",
        "try:\n",
        "  column_name= client.query(f'''\n",
        "                SELECT\n",
        "                * \n",
        "                FROM\n",
        "                `{project_id}`.kbar.INFORMATION_SCHEMA.COLUMNS\n",
        "                WHERE table_name= {table_id}''').to_dataframe().column_name.values.tolist()\n",
        "except:\n",
        "  column_name = []\n",
        "\n",
        "#允許新增欄位\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "            destination= f\"{project_id}.{dataset}.{table_id}\",\n",
        "            schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
        "            write_disposition= bigquery.WriteDisposition.WRITE_APPEND,\n",
        "            )\n",
        "\n",
        "#查詢最新更新日期\n",
        "try:\n",
        "  update_time = pd.read_gbq(f'''\n",
        "                SELECT\n",
        "                #min(ts),\n",
        "                max(ts)\n",
        "                FROM\n",
        "                `{project_id}.{dataset}.{table_id}`\n",
        "                ''')\n",
        "  last_update = pd.to_datetime(update_time.f1_.values[0])#.strftime('%Y-%m-%d')\n",
        "  init_update = pd.to_datetime(update_time.f0_.values[0])\n",
        "except:\n",
        "  last_update = pd.to_datetime('2018-12-06') #如果沒有資料，設定成可下載期間初始日的前一日，在計算回推日期時，初始點(point_str)才是正確的\n",
        "  #init_update = pd.to_datetime('2018-12-06')\n",
        "\n",
        "if not column_name:      #如果沒有現有欄位，則逐批重新抓取\n",
        "  for date_begin, date_end in zip(list_date[0::200], list_date[199::200]):\n",
        "    new += 1\n",
        "\n",
        "    close_5min = pd.DataFrame([])\n",
        "    open_5min = pd.DataFrame([])\n",
        "    high_5min = pd.DataFrame([])\n",
        "    low_5min = pd.DataFrame([])\n",
        "    volume_5min = pd.DataFrame([])\n",
        "    close_day = pd.DataFrame([])\n",
        "    open_day = pd.DataFrame([])\n",
        "    high_day = pd.DataFrame([])\n",
        "    low_day = pd.DataFrame([])\n",
        "    volume_day = pd.DataFrame([])\n",
        "  \n",
        "    for code in tq.tqdm(stocks_table.code, desc= 'code loop', position= 0):\n",
        "      kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= date_begin.strftime('%Y-%m-%d'), end= date_end.strftime('%Y-%m-%d'))\n",
        "      df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "      df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "          'Open': 'first', \n",
        "          'High': 'max', \n",
        "          'Low': 'min', \n",
        "          'Close': 'last',\n",
        "          'Volume': 'sum'}).replace(0, np.NaN).dropna(how= 'all')\n",
        "      #df_day = df.resample(rule='D').agg({\n",
        "      #    'Open': 'first', \n",
        "      #    'High': 'max', \n",
        "      #    'Low': 'min', \n",
        "      #    'Close': 'last',\n",
        "      #    'Volume': 'sum'}).replace(0, np.NaN).dropna(how= 'all')\n",
        "\n",
        "      df_5min.sort_index(inplace= True)\n",
        "      #df_day.sort_index(inplace= True)\n",
        "      #5min\n",
        "      #close\n",
        "      close_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Close\n",
        "      close_5min.index.rename('ts', inplace= True)\n",
        "      #open\n",
        "      open_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Open\n",
        "      open_5min.index.rename('ts', inplace= True)\n",
        "      #high\n",
        "      high_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.High\n",
        "      high_5min.index.rename('ts', inplace= True)\n",
        "      #low\n",
        "      low_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Low\n",
        "      low_5min.index.rename('ts', inplace= True)    \n",
        "      #volume\n",
        "      volume_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Volume\n",
        "      volume_5min.index.rename('ts', inplace= True)\n",
        "\n",
        "      #day\n",
        "      #close\n",
        "      close_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Close\n",
        "      close_day.index.rename('ts', inplace= True)\n",
        "      #open\n",
        "      open_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Open\n",
        "      open_day.index.rename('ts', inplace= True)\n",
        "      #high\n",
        "      high_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.High\n",
        "      high_day.index.rename('ts', inplace= True)\n",
        "      #low\n",
        "      low_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Low\n",
        "      low_day.index.rename('ts', inplace= True)\n",
        "      #volume\n",
        "      volume_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Volume\n",
        "      volume_day.index.rename('ts', inplace= True)\n",
        "else:\n",
        "  if last_update.date() == list_date[-1]:\n",
        "    sys.stdout.write('已是最新資料')\n",
        "    sys.stdout.flush()\n",
        "  else:\n",
        "    point_str = -(sum(last_update < list_date))\n",
        "    sys.stdout.write('\\r' + f\"須更新{list_date[point_str]}後的資料\")\n",
        "    \n",
        "    for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol in column_name]].code, desc= 'existing column updating', position= 1):\n",
        "      kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= list_date[point_str].strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "      df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "      df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "          'Open': 'first', \n",
        "          'High': 'max', \n",
        "          'Low': 'min', \n",
        "          'Close': 'last',\n",
        "          'Volume': 'sum'}).replace(0, np.NaN).dropna(how= 'all')\n",
        "      df_5min.sort_index(inplace= True)\n",
        "      close_5min['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "      close_5min.index.rename('ts', inplace= True)\n",
        "      sys.stdout.write('\\r' + f'還剩下{len(column_name) - n}待更新，並須插入{len(stocks_table.index) - len(column_name) - new}項')\n",
        "      sys.stdout.flush()\n",
        "    for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol not in column_name]].code, desc= 'new column', position= 1):\n",
        "      kbars_to_add = api.kbars(api.Contracts.Stocks[f'{code}'], start= list_date[0].strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "      df_to_add = pd.DataFrame({**kbars_to_add}, index= pd.to_datetime(kbars_to_add.ts)).drop('ts', axis= 1)\n",
        "      df_5min_to_add = df_to_add.resample(rule='5Min', closed='right').agg({\n",
        "          'Open': 'first', \n",
        "          'High': 'max', \n",
        "          'Low': 'min', \n",
        "          'Close': 'last',\n",
        "          'Volume': 'sum'}).replace(0, np.NaN)#.dropna(how= 'all')\n",
        "      df_5min_to_add.sort_index(inplace= True)\n",
        "      close_5min_to_add['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.Close\n",
        "      close_5min_to_add.index.rename('ts', inplace= True)\n",
        "      new += 1\n",
        "      sys.stdout.write('\\r' + f'已插入{new}項，還剩下{len(stocks_table.index) - len(column_name) - new}項')\n",
        "      sys.stdout.flush()\n",
        "    \n",
        "if not close_5min.empty:\n",
        "  close_5min.reset_index().to_gbq(f'{dataset}.close_5min', project_id= project_id, if_exists='append')\n",
        "  open_5min.reset_index().to_gbq(f'{dataset}.open_5min', project_id= project_id, if_exists='append')\n",
        "  high_5min.reset_index().to_gbq(f'{dataset}.high_5min', project_id= project_id, if_exists='append')\n",
        "  low_5min.reset_index().to_gbq(f'{dataset}.low_5min', project_id= project_id, if_exists='append')\n",
        "  volume_5min.reset_index().to_gbq(f'{dataset}.volume_5min', project_id= project_id, if_exists='append')\n",
        "\n",
        "  close_day.reset_index().to_gbq(f'{dataset}_day.close', project_id= project_id, if_exists='append')\n",
        "  open_day.reset_index().to_gbq(f'{dataset}_day.open', project_id= project_id, if_exists='append')\n",
        "  high_day.reset_index().to_gbq(f'{dataset}_day.high', project_id= project_id, if_exists='append')\n",
        "  low_day.reset_index().to_gbq(f'{dataset}_day.low', project_id= project_id, if_exists='append')\n",
        "  volume_day.reset_index().to_gbq(f'{dataset}_day.volume', project_id= project_id, if_exists='append')\n",
        "\n",
        "if not close_5min_to_add.empty:\n",
        "    table = client.get_table(f'{dataset}.close_5min')  # Make an API request.\n",
        "\n",
        "    original_schema = table.schema\n",
        "    new_schema = original_schema[:]  # Creates a copy of the schema.\n",
        "    for new_columns in [*close_5min_to_add.columns]:\n",
        "      new_schema.append(bigquery.SchemaField(f'{new_columns}', 'FLOAT'))\n",
        "    #for new_columns in close_5min_to_add.columns:\n",
        "    #  client.query(f'''\n",
        "    #          INSERT \n",
        "    #          `{dataset}.{table_id}` ({new_columns})\n",
        "    #          VALUES\n",
        "    #          UNNEST({[close_5min_to_add['{new_columns}'].values]})\n",
        "    #          WHERE ts = {close_5min_to_add['{new_columns}'].ts}\n",
        "    #        ''')\n",
        "\n",
        "    table.schema = new_schema\n",
        "    table = client.update_table(table, [\"schema\"])  # Make an API request.\n",
        "\n",
        "    if len(table.schema) == len(original_schema) + 1 == len(new_schema):\n",
        "      print(\"A new column has been added.\")\n",
        "    else:\n",
        "      print(\"The column has not been added.\")\n",
        "      # Start the query, passing in the extra configuration.\n",
        "    client.load_table_from_dataframe(close_5min_to_add, f'{dataset}.close_5min')\n",
        "\n",
        "    print('已完成{}項'.format(new))\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ver.2"
      ],
      "metadata": {
        "id": "PgueYH0wkxae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm.notebook as tq\n",
        "import sys\n",
        "n= 0\n",
        "new= 0\n",
        "\n",
        "#允許新增欄位\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    destination= f\"{project_id}.kbar.close_5min\",\n",
        "    schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
        "    write_disposition= bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    )\n",
        "#查詢最新更新日期\n",
        "try:\n",
        "  update_time = pd.read_gbq(f'''\n",
        "                SELECT\n",
        "                min(ts),\n",
        "                max(ts)\n",
        "                FROM\n",
        "                `{project_id}.kbar.close_5min`\n",
        "                ''')\n",
        "  last_update = pd.to_datetime(update_time.f1_.values[0])#.strftime('%Y-%m-%d')\n",
        "  init_update = pd.to_datetime(update_time.f0_.values[0])\n",
        "except:\n",
        "  last_update = pd.to_datetime('2018-12-06') #如果沒有資料，設定成可下載期間初始日的前一日，在計算回推日期時，初始點(point_str)才是正確的\n",
        "  init_update = pd.to_datetime('2018-12-06')\n",
        "##\n",
        "close_5min = pd.DataFrame([])\n",
        "close_5min_to_add = pd.DataFrame([])\n",
        "for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol in column_name]].code):\n",
        "  if last_update.date() == list_date[-1]:\n",
        "    print('已是最新資料')\n",
        "    break\n",
        "  else:\n",
        "    n += 1\n",
        "    point_str = -(sum(last_update < list_date))\n",
        "    sys.stdout.write('\\r' + f\"須更新{list_date[point_str]}後的資料\", end = \"\\r\")\n",
        "    kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= list_date[point_str].strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "    df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "    df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'}).replace(0, np.NaN).dropna(how= 'all')\n",
        "    df_5min.sort_index(inplace= True)\n",
        "    close_5min['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min.Close\n",
        "    close_5min.index.rename('ts', inplace= True)\n",
        "    sys.stdout.write('\\r' + f'還剩下{len(column_name) - n}待更新，並須插入{len(stocks_table.index) - len(column_name) - new}項')\n",
        "    sys.stdout.flush()\n",
        "if [symbol for symbol in stocks_table.index if symbol not in column_name]:\n",
        "  for code in tq.tqdm(stocks_table.loc[[symbol for symbol in stocks_table.index if symbol not in column_name]].code):\n",
        "    kbars_to_add = api.kbars(api.Contracts.Stocks[f'{code}'], start= list_date[0].strftime('%Y-%m-%d'), end= list_date[-1].strftime('%Y-%m-%d'))\n",
        "    df_to_add = pd.DataFrame({**kbars_to_add}, index= pd.to_datetime(kbars_to_add.ts)).drop('ts', axis= 1)\n",
        "    df_5min_to_add = df_to_add.resample(rule='5Min', closed='right').agg({\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'}).replace(0, np.NaN)#.dropna(how= 'all')\n",
        "    df_5min_to_add.sort_index(inplace= True)\n",
        "    close_5min_to_add['{}'.format(stocks_table[stocks_table.code == code].index[0])] = df_5min_to_add.Close\n",
        "    close_5min_to_add.index.rename('ts', inplace= True)\n",
        "    sys.stdout.write('\\r' + f'已須插入{new}項，還剩下{len(stocks_table.index) - len(column_name) - new}項')\n",
        "    new += 1\n",
        "    sys.stdout.flush()\n",
        "if not close_5min.empty:\n",
        "  pd_gbq.to_gbq(close_5min.reset_index(), f'{dataset}.close_5min', if_exists= 'append')\n",
        "\n",
        "if not close_5min_to_add.empty:\n",
        "  table = client.get_table(f'{dataset}.close_5min')  # Make an API request.\n",
        "\n",
        "  original_schema = table.schema\n",
        "  new_schema = original_schema[:]  # Creates a copy of the schema.\n",
        "  #for new_columns in [*close_5min_to_add.columns]:\n",
        "  #  new_schema.append(bigquery.SchemaField(f'{new_columns}', 'FLOAT'))\n",
        "  #for new_columns in close_5min_to_add.columns:\n",
        "    #  client.query(f'''\n",
        "    #          INSERT \n",
        "    #          `{dataset}.{table_id}` ({new_columns})\n",
        "    #          VALUES\n",
        "    #          UNNEST({[close_5min_to_add['{new_columns}'].values]})\n",
        "    #          WHERE ts = {close_5min_to_add['{new_columns}'].ts}\n",
        "    #        ''')\n",
        "\n",
        "  #table.schema = new_schema\n",
        "  #table = client.update_table(table, [\"schema\"])  # Make an API request.\n",
        "\n",
        "  #if len(table.schema) == len(original_schema) + 1 == len(new_schema):\n",
        "  #  print(\"A new column has been added.\")\n",
        "  #else:\n",
        "  #  print(\"The column has not been added.\")\n",
        "    # Start the query, passing in the extra configuration.\n",
        "  client.load_table_from_dataframe(close_5min_to_add, f'{dataset}.close_5min')\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "qfpFziu3cD9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ver.1(完全從頭開始抓資料)\n"
      ],
      "metadata": {
        "id": "GUIv-kaGvb4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7M6c7YH8FI7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "n= 0\n",
        "\n",
        "for date_begin, date_end in zip(list_date[1::20], list_date[20::20]):\n",
        "  if date_begin == pd.to_datetime('2018-12-10'):\n",
        "    date_begin = pd.to_datetime('2018-12-07')\n",
        "  else:\n",
        "    pass\n",
        "  n += 1\n",
        "  close_5min = pd.DataFrame([])\n",
        "#  open_5min = pd.DataFrame([])\n",
        "#  high_5min = pd.DataFrame([])\n",
        "#  low_5min = pd.DataFrame([])\n",
        "#  volume_5min = pd.DataFrame([])\n",
        "#  close_day = pd.DataFrame([])\n",
        "#  open_day = pd.DataFrame([])\n",
        "#  high_day = pd.DataFrame([])\n",
        "#  low_day = pd.DataFrame([])\n",
        "#  volume_day = pd.DataFrame([])\n",
        "#  close_week = pd.DataFrame([])\n",
        "#  open_week = pd.DataFrame([])\n",
        "#  high_week = pd.DataFrame([])\n",
        "#  low_week = pd.DataFrame([])\n",
        "#  volume_week = pd.DataFrame([])\n",
        "\n",
        "  for code in stocks_table.code:\n",
        "    kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start= date_begin.strftime('%Y-%m-%d'), end= date_end.strftime('%Y-%m-%d'))\n",
        "    df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "    df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'}).replace(0, np.NaN).dropna(how= 'all')\n",
        " #   df_day = df.resample(rule='D').agg({\n",
        " #         'Open': 'first', \n",
        " #         'High': 'max', \n",
        " #         'Low': 'min', \n",
        " #         'Close': 'last',\n",
        " #         'Volume': 'sum'}).replace(0, np.NaN).dropna(how= 'all')\n",
        "\n",
        "    df_5min.sort_index(inplace= True)\n",
        " #   df_day.sort_index(inplace= True)\n",
        "#    #5min\n",
        "#    #close\n",
        "    close_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Close\n",
        "    close_5min.index.rename('ts', inplace= True)\n",
        "#    #open\n",
        "#    open_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Open\n",
        "#    open_5min.index.rename('ts', inplace= True)\n",
        "#    #high\n",
        "#    high_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.High\n",
        "#    high_5min.index.rename('ts', inplace= True)\n",
        "#    #low\n",
        "#    low_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Low\n",
        "#    low_5min.index.rename('ts', inplace= True)    \n",
        "#    #volume\n",
        "#    volume_5min['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_5min.Volume\n",
        "#    volume_5min.index.rename('ts', inplace= True)\n",
        "#\n",
        "#    #day\n",
        "#    #close\n",
        "#    close_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Close\n",
        "#    close_day.index.rename('ts', inplace= True)\n",
        "#    #open\n",
        "#    open_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Open\n",
        "#    open_day.index.rename('ts', inplace= True)\n",
        "#    #high\n",
        "#    high_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.High\n",
        "#    high_day.index.rename('ts', inplace= True)\n",
        "#    #low\n",
        "#    low_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Low\n",
        "#    low_day.index.rename('ts', inplace= True)\n",
        "#    #volume\n",
        "#    volume_day['{}'.format(stocks_table[stocks_table.code == code].symbol.to_list()[0])] = df_day.Volume\n",
        "#    volume_day.index.rename('ts', inplace= True)\n",
        "  \n",
        "  \n",
        "  close_5min.reset_index().to_gbq(f'{dataset}.close_5min', project_id= project_id, if_exists='append')\n",
        "#  open_5min.reset_index().to_gbq(f'{dataset}.open_5min', project_id= project_id, if_exists='append')\n",
        "#  high_5min.reset_index().to_gbq(f'{dataset}.high_5min', project_id= project_id, if_exists='append')\n",
        "#  low_5min.reset_index().to_gbq(f'{dataset}.low_5min', project_id= project_id, if_exists='append')\n",
        "#  volume_5min.reset_index().to_gbq(f'{dataset}.volume_5min', project_id= project_id, if_exists='append')\n",
        "  display(close_5min)\n",
        "#\n",
        "#  close_day.reset_index().to_gbq(f'{dataset}_day.close', project_id= project_id, if_exists='append')\n",
        "#  open_day.reset_index().to_gbq(f'{dataset}_day.open', project_id= project_id, if_exists='append')\n",
        "#  high_day.reset_index().to_gbq(f'{dataset}_day.high', project_id= project_id, if_exists='append')\n",
        "#  low_day.reset_index().to_gbq(f'{dataset}_day.low', project_id= project_id, if_exists='append')\n",
        "#  volume_day.reset_index().to_gbq(f'{dataset}_day.volume', project_id= project_id, if_exists='append')\n",
        "\n",
        "  print('已完成{}項'.format(n))\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4esaaQUell4"
      },
      "source": [
        "##下載K棒資料並存入GBQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoJzTppSG_UM"
      },
      "outputs": [],
      "source": [
        "database_name = kbar_data\n",
        "count = 0\n",
        "for exchange in stock:\n",
        "    for stocks in exchange:\n",
        "        if stocks.exchange in (Exchange.TSE, Exchange.OTC):\n",
        "            if stocks.code in set(existing_code): #跳過已經下載過的資料\n",
        "                continue \n",
        "            if stocks.category == '00' or stocks.category == '':\n",
        "                continue\n",
        "            elif pd.to_datetime(stocks.update_date) != LAST_TRADE_DATE:\n",
        "                continue\n",
        "            elif re.search('[A-Z]', stocks.code) is None:  \n",
        "              count += 1\n",
        "              print(f'{count}. start download <{stocks.code} {stocks.name}> kbar data...')\n",
        "              kbars = api.kbars(stocks, start='2018-12-07', end=TODAY)\n",
        "              df = pd.DataFrame({**kbars})\n",
        "              df.ts = pd.to_datetime(df.ts)\n",
        "              df['Code'] = stocks.code\n",
        "              df['Name'] = stocks.name\n",
        "              df.to_gbq(destination_table= 'my_database.{}'.format(database_name), project_id= project_id, if_exists='append')\n",
        "              print(f'<{stocks.code} {stocks.name}> kbar data is stored to Google BigQuery' + ' 目前進度{:.2f}%'.format((count/(len(df_stock) - len(existing_code)))*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-h1UqVd3d5"
      },
      "source": [
        "## Resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW0A7h9zgVQ9"
      },
      "outputs": [],
      "source": [
        "from shioaji.constant import Exchange\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from talib.abstract import *\n",
        "import numpy as np\n",
        "\n",
        "contracts = api.Contracts\n",
        "stock = contracts.Stocks\n",
        "future = contracts.Futures\n",
        "\n",
        "# 設定各頻率資料表名稱\n",
        "table_name_5min = 'my_database.kbars_5min'\n",
        "table_name_day = 'my_database.kbar_day'\n",
        "table_name_week = 'my_database.kbar_week'\n",
        "#table_name_last_15 = 'my_database.kbars_last_15'\n",
        "\n",
        "# 傳入股票代碼，產生5分K資料並存至資料庫\n",
        "\n",
        "def kbar_5min(stock_code, start = \"2018-12-07\"):\n",
        "    print(f'generate 5minK for stock:<{stock_code}> {stock[stock_code].name}') #輸出程式執行點資料至console\n",
        "    df = pd.DataFrame({**api.kbars(stock[stock_code], start= start,)}).set_index('ts')\n",
        "    ##pd_gbq.read_gbq('SELECT Code, Name, Open, High, Low, Close, Volume, ts FROM my_database.{} WHERE Code = \"{}\"'.format(table_name, stock_code) , project_id= project_id, index_col='ts')\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df_5min_kbar = df.resample(rule='5Min', closed='right').agg({\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'}).dropna() #須去除NA，否則可能出現格式錯誤\n",
        "    df_5min_kbar['Code'] = stock_code\n",
        "    df_5min_kbar['Name'] = stock[stock_code].name\n",
        "    # 技術指標\n",
        "#    close = df_5min_kbar.Close\n",
        "#    high = df_5min_kbar.High\n",
        "#    low = df_5min_kbar.Low\n",
        "#    df_5min_kbar['MA5'] = close.rolling(5).mean()\n",
        "#    df_5min_kbar['MA10'] = close.rolling(10).mean()\n",
        "#    df_5min_kbar['MA20'] = close.rolling(20).mean()\n",
        "#    df_5min_kbar['MA60'] = close.rolling(60).mean()\n",
        "#    df_5min_kbar['UB'] = BBANDS(close, 20, 2.0, 2.0)[0]\n",
        "#    df_5min_kbar['MB'] = BBANDS(close, 20, 2.0, 2.0)[1]\n",
        "#    df_5min_kbar['LB'] = BBANDS(close, 20, 2.0, 2.0)[2]\n",
        "\n",
        "\n",
        "    df_5min_kbar.fillna(0, inplace= True)\n",
        "    df_5min_kbar.sort_index()\n",
        "\n",
        "    #last_15 = pd.DataFrame(close[(close.index.hour == 13)&(close.index.minute >= 15)]) # 晚於每個交易日13點15分的資料\n",
        "    #last_15['Code'] = stock[stock_code].code\n",
        "    #last_15['Name'] = stock[stock_code].name\n",
        "\n",
        "    df_5min_kbar.reset_index(inplace= True)\n",
        "    #last_15.reset_index(inplace= True)\n",
        "    df_5min_kbar.to_gbq(f'{table_name_5min}', project_id= project_id, if_exists='append')\n",
        "    #last_15.to_gbq(f'{table_name_last_15}', project_id= project_id, if_exists='append')\n",
        "    print(f'stock:{stock_code}, 5minK is added to DB...') #輸出程式執行點資料至console\n",
        "\n",
        "def kbar_day(stock_code, start = \"2018-12-07\"):\n",
        "    print(f'generate DayK for stock:<{stock_code}> {stock[stock_code].name}') #輸出程式執行點資料至console\n",
        "    df = pd.DataFrame({**api.kbars(stock[stock_code], start= start)}).set_index('ts')\n",
        "    ##pd_gbq.read_gbq('SELECT Code, Name, Open, High, Low, Close, Volume, ts FROM my_database.{} WHERE Code = \"{}\"'.format(table_name, stock_code) , project_id= project_id, index_col='ts')\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df_day_kbar = df.resample(rule='D').agg({\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'}).dropna()\n",
        "    df_day_kbar['Code'] = stock_code\n",
        "    df_day_kbar['Name'] = stock[stock_code].name\n",
        "    # 技術指標\n",
        "#    close = df_day_kbar.Close\n",
        "#    high = df_day_kbar.High\n",
        "#    low = df_day_kbar.Low\n",
        "#    df_day_kbar['MA5'] = close.rolling(5).mean()\n",
        "#    df_day_kbar['MA10'] = close.rolling(10).mean()\n",
        "#    df_day_kbar['MA20'] = close.rolling(20).mean()\n",
        "#    df_day_kbar['MA60'] = close.rolling(60).mean()\n",
        "#    df_day_kbar['UB'] = BBANDS(close, 20, 2.0, 2.0)[0]\n",
        "#    df_day_kbar['MB'] = BBANDS(close, 20, 2.0, 2.0)[1]\n",
        "#    df_day_kbar['LB'] = BBANDS(close, 20, 2.0, 2.0)[2]\n",
        "    \n",
        "    df_day_kbar.fillna(0, inplace= True)\n",
        "    df_day_kbar.dropna(axis=0, inplace=True)\n",
        "    df_day_kbar.sort_index()\n",
        "    df_day_kbar.reset_index(inplace= True)\n",
        "    df_day_kbar.to_gbq(f'{table_name_day}', project_id= project_id, if_exists='append',)\n",
        "    print(f'stock:{stock_code}, DayK is added to DB...') #輸出程式執行點資料至console\n",
        "\n",
        "def kbar_week(stock_code, start= \"2018-12-07\"):\n",
        "    print(f'generate week_K for stock:<{stock_code}> {stock[stock_code].name}') #輸出程式執行點資料至console\n",
        "    df = pd.DataFrame({**api.kbars(stock[stock_code], start= start)}).set_index('ts')\n",
        "    ##pd_gbq.read_gbq('SELECT Code, Name, Open, High, Low, Close, Volume, ts FROM my_database.{} WHERE Code = \"{}\"'.format(table_name, stock_code) , project_id= project_id, index_col='ts')\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df_week_kbar = df.resample(rule='W').agg({\n",
        "        'Open': 'first', \n",
        "        'High': 'max', \n",
        "        'Low': 'min', \n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'}).dropna()\n",
        "    df_week_kbar['Code'] = stock_code\n",
        "    df_week_kbar['Name'] = stock[stock_code].name\n",
        "\n",
        "#    close = df_week_kbar.Close\n",
        "#    high = df_week_kbar.High\n",
        "#    low = df_week_kbar.Low\n",
        "#    df_week_kbar['MA5'] = close.rolling(5).mean()\n",
        "#    df_week_kbar['MA10'] = close.rolling(10).mean()\n",
        "#    df_week_kbar['MA20'] = close.rolling(20).mean()\n",
        "#    df_week_kbar['MA60'] = close.rolling(60).mean()\n",
        "#    df_week_kbar['UB'] = BBANDS(close, 20, 2.0, 2.0)[0]\n",
        "#    df_week_kbar['MB'] = BBANDS(close, 20, 2.0, 2.0)[1]\n",
        "#    df_week_kbar['LB'] = BBANDS(close, 20, 2.0, 2.0)[2]\n",
        "    \n",
        "    df_week_kbar.fillna(0, inplace= True)\n",
        "    df_week_kbar.dropna(axis=0, inplace=True)\n",
        "    df_week_kbar.sort_index()\n",
        "    df_week_kbar.reset_index(inplace= True)\n",
        "    df_week_kbar.to_gbq(f'{table_name_week}', project_id= project_id, if_exists='append',)\n",
        "    print(f'stock:{stock_code}, week_K is added to DB...') #輸出程式執行點資料至console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfnQVir5OlI2",
        "outputId": "be371abf-8290-4f7d-90f4-4aad22c5397f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 1741/1741 [00:00<00:00, 5912.56rows/s]\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "start = '2018-12-07'\n",
        "TODAY = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "LAST_TRADE_DATE = max(df_stock.update_date)\n",
        "list_code = []\n",
        "update_date = []\n",
        "for exchange in stock:\n",
        "    for stocks in exchange:\n",
        "        if stocks.exchange in (Exchange.TSE, Exchange.OTC):\n",
        "          if stocks.category == '00' or stocks.category == '':\n",
        "                continue\n",
        "          elif stocks.update_date != LAST_TRADE_DATE: #剔除已下市股票\n",
        "                continue\n",
        "          elif re.search('[A-Z]', stocks.code) is None:\n",
        "            list_code.append(stocks.code)\n",
        "            update_date.append(stocks.update_date)\n",
        "#list_code = list_stock.Code\n",
        "LAST_TRADE_DATE = max(update_date)\n",
        "df = pd.DataFrame([], index= None, columns=['Open', 'High', 'Low', 'Close', 'Volume', 'Code'])\n",
        "\n",
        "#-------------------5min----------------------\n",
        "try:\n",
        "  existing_kbar_5min = pd_gbq.read_gbq(\n",
        "    f'''                #3個引號才有辦法分行讀\n",
        "    SELECT Code, \n",
        "    Max(ts) as last_update, \n",
        "#    Min(ts) as init_update \n",
        "    FROM {table_name_5min} \n",
        "    GROUP BY Code\n",
        "    '''\n",
        "    , project_id= project_id,\n",
        "    dtypes= [{'name': 'code', 'type': 'STRING'}, {'name': 'last_update', 'type': 'date'}]) \n",
        "except:\n",
        "  existing_kbar_5min = df\n",
        "\n",
        "#-------------------day----------------------\n",
        "try:\n",
        "  existing_kbar_day = pd_gbq.read_gbq(\n",
        "    f'''\n",
        "    SELECT Code, \n",
        "    Max(ts) as last_update, \n",
        "#    Min(ts) as init_update \n",
        "    FROM {table_name_day} \n",
        "    GROUP BY Code\n",
        "    '''\n",
        "    , project_id= project_id,\n",
        "    dtypes= [{'name': 'code', 'type': 'STRING'}, {'name': 'last_update', 'type': 'date'}])\n",
        "except:\n",
        "  existing_kbar_day = df\n",
        "\n",
        "#-------------------week----------------------\n",
        "try:\n",
        "  existing_kbar_week = pd_gbq.read_gbq(\n",
        "    f'''\n",
        "    SELECT Code, \n",
        "    Max(ts) as last_update, \n",
        "#    Min(ts) as init_update \n",
        "    FROM {table_name_week} \n",
        "    GROUP BY Code\n",
        "    '''\n",
        "    , project_id= project_id,\n",
        "    dtypes= [{'name': 'code', 'type': 'STRING'}, {'name': 'last_update', 'type': 'date'}])\n",
        "except:\n",
        "  existing_kbar_week = df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 擷取所有欄位名稱(查一次要10MB，省著點用)\n",
        "column_name= client.query(f'''\n",
        "              SELECT\n",
        "              * \n",
        "              FROM\n",
        "              `{project_id}`.kbar.INFORMATION_SCHEMA.COLUMNS\n",
        "              WHERE table_name=\"close_5min\"''').to_dataframe().column_name.values.tolist()\n",
        "print(column_name, '\\n', '總共有{}項'.format(len(column_name[1:])))"
      ],
      "metadata": {
        "id": "c0XckmuHr4Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efc72aa-053a-4f11-d786-34b41c5678e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ts', 'OTC6612', 'OTC8034', 'OTC5487', 'OTC5386', 'OTC2745', 'OTC2724', 'OTC4735', 'OTC1569', 'OTC1565', 'OTC3594', 'OTC3083', 'OTC6570', 'OTC6210', 'OTC4726', 'OTC2752', 'OTC4533', 'OTC5223', 'OTC4113', 'OTC4534', 'OTC6169', 'OTC8087', 'OTC8099', 'OTC8076', 'OTC8437', 'OTC3228', 'OTC8424', 'OTC5543', 'OTC8908', 'OTC1336', 'OTC6527', 'OTC5475', 'OTC5864', 'OTC3562', 'OTC2729', 'OTC3088', 'OTC3252', 'OTC8418', 'OTC8071', 'OTC2755', 'OTC3265', 'OTC6568', 'OTC2718', 'OTC6624', 'OTC6245', 'OTC3260', 'OTC8084', 'OTC5480', 'OTC4974', 'OTC5016', 'OTC6425', 'OTC3587', 'OTC6712', 'OTC5488', 'OTC5281', 'OTC5014', 'OTC6247', 'OTC5383', 'OTC5324', 'OTC5601', 'OTC3611', 'OTC6163', 'OTC4138', 'OTC2067', 'OTC4609', 'OTC6419', 'OTC6227', 'OTC3564', 'OTC8085', 'OTC3360', 'OTC6109', 'OTC1777', 'OTC8435', 'OTC5468', 'OTC3556', 'OTC9950', 'OTC8930', 'OTC5353', 'OTC1570', 'OTC6589', 'OTC1333', 'OTC3379', 'OTC8077', 'OTC3630', 'OTC4430', 'OTC5287', 'OTC6752', 'OTC6576', 'OTC3548', 'OTC3323', 'OTC4147', 'OTC3580', 'OTC1742', 'OTC5511', 'OTC1784', 'OTC8941', 'OTC4121', 'OTC5512', 'OTC3310', 'OTC5306', 'OTC6023', 'OTC3625', 'OTC6482', 'OTC3089', 'OTC8088', 'OTC3227', 'OTC6231', 'OTC6732', 'OTC5701', 'OTC4554', 'OTC8067', 'OTC4527', 'OTC8905', 'OTC3646', 'OTC3511', 'OTC6161', 'OTC8921', 'OTC3713', 'OTC4154', 'OTC6143', 'OTC3294', 'OTC6218', 'OTC1591', 'OTC5212', 'OTC8047', 'OTC3693', 'OTC8406', 'OTC4535', 'OTC6171', 'OTC4549', 'OTC3362', 'OTC4972', 'OTC5465', 'OTC3288', 'OTC4123', 'OTC8450', 'OTC8455', 'OTC6198', 'OTC4163', 'OTC3523', 'OTC6287', 'OTC2063', 'OTC8059', 'OTC8431', 'OTC4973', 'OTC4953', 'OTC3491', 'OTC5364', 'OTC6613', 'OTC8476', 'OTC8446', 'OTC4806', 'OTC5351', 'OTC3527', 'OTC4432', 'OTC5301', 'OTC2924', 'OTC6160', 'OTC4706', 'OTC5490', 'OTC8097', 'OTC8043', 'OTC4107', 'OTC5299', 'OTC3115', 'OTC8932', 'OTC3438', 'OTC2726', 'OTC6596', 'OTC3141', 'OTC8289', 'OTC4716', 'OTC2937', 'OTC8155', 'OTC6154', 'OTC6446', 'OTC3570', 'OTC8074', 'OTC3176', 'OTC4503', 'OTC6237', 'OTC6020', 'OTC4131', 'OTC4128', 'OTC8069', 'OTC6496', 'OTC6126', 'OTC4160', 'OTC4907', 'OTC5704', 'OTC8913', 'OTC6261', 'OTC3268', 'OTC2061', 'OTC4744', 'OTC6485', 'OTC6411', 'OTC6182', 'OTC3317', 'OTC8416', 'OTC3529', 'OTC6733', 'OTC5460', 'OTC5314', 'OTC3289', 'OTC3339', 'OTC4139', 'OTC6590', 'OTC5443', 'OTC7556', 'OTC3064', 'OTC4905', 'OTC4188', 'OTC6472', 'OTC8444', 'OTC6101', 'OTC4903', 'OTC5902', 'OTC8924', 'OTC6263', 'OTC3642', 'OTC8049', 'OTC3303', 'OTC5425', 'OTC5371', 'OTC1796', 'OTC4502', 'OTC4406', 'OTC3441', 'OTC8927', 'OTC5703', 'OTC6469', 'OTC2719', 'OTC5439', 'OTC6462', 'OTC3522', 'OTC3521', 'OTC6651', 'OTC6016', 'OTC3687', 'OTC4741', 'OTC5381', 'OTC4747', 'OTC6180', 'OTC6138', 'OTC5291', 'OTC4111', 'OTC3537', 'OTC3221', 'OTC6747', 'OTC6220', 'OTC5903', 'OTC5489', 'OTC3191', 'OTC3287', 'OTC3675', 'OTC6664', 'OTC6222', 'OTC3259', 'OTC3230', 'OTC3128', 'OTC6855', 'OTC5536', 'OTC6246', 'OTC6684', 'OTC6762', 'OTC6667', 'OTC6229', 'OTC4931', 'OTC5213', 'OTC8183', 'OTC2066', 'OTC4721', 'OTC6212', 'OTC6547', 'OTC6294', 'OTC6124', 'OTC6767', 'OTC5312', 'OTC3628', 'OTC6506', 'OTC1595', 'OTC6616', 'OTC6727', 'OTC4429', 'OTC5340', 'OTC4416', 'OTC8277', 'OTC6127', 'OTC3685', 'OTC8171', 'OTC4205', 'OTC6194', 'OTC3066', 'OTC8054', 'OTC3680', 'OTC6654', 'OTC6204', 'OTC4568', 'OTC3577', 'OTC6111', 'OTC3226', 'OTC6509', 'OTC4702', 'OTC6147', 'OTC5603', 'OTC4129', 'OTC6574', 'OTC6532', 'OTC8032', 'OTC8182', 'OTC1788', 'OTC5438', 'OTC6578', 'OTC5457', 'OTC8291', 'OTC1781', 'OTC3567', 'OTC3105', 'OTC3465', 'OTC1580', 'OTC8401', 'OTC6703', 'OTC8024', 'OTC8255', 'OTC2947', 'OTC4413', 'OTC5345', 'OTC2230', 'OTC6486', 'OTC6492', 'OTC2643', 'OTC5604', 'OTC4114', 'OTC3374', 'OTC4102', 'OTC6170', 'OTC5878', 'OTC6236', 'OTC8935', 'OTC6857', 'OTC4939', 'OTC5452', 'OTC6026', 'OTC5347', 'OTC2064', 'OTC3512', 'OTC3629', 'OTC1584', 'OTC8421', 'OTC8080', 'OTC6609', 'OTC4995', 'OTC8121', 'OTC3178', 'OTC4736', 'OTC3144', 'OTC8342', 'OTC8234', 'OTC6560', 'OTC3213', 'OTC6662', 'OTC3479', 'OTC6441', 'OTC3597', 'OTC8936', 'OTC4707', 'OTC6661', 'OTC8916', 'OTC4754', 'OTC6021', 'OTC8111', 'OTC6470', 'OTC3632', 'OTC9951', 'OTC6516', 'OTC8383', 'OTC5464', 'OTC6556', 'OTC4712', 'OTC4401', 'OTC6244', 'OTC5481', 'OTC8299', 'OTC5356', 'OTC8923', 'OTC4908', 'OTC8489', 'OTC3402', 'OTC8931', 'OTC6640', 'OTC8409', 'OTC3666', 'OTC5410', 'OTC8440', 'OTC8064', 'OTC5529', 'OTC5478', 'OTC6535', 'OTC3354', 'OTC1258', 'OTC3325', 'OTC8083', 'OTC6683', 'OTC8432', 'OTC8410', 'OTC8349', 'OTC8436', 'OTC1787', 'OTC3162', 'OTC3217', 'OTC3284', 'OTC3264', 'OTC1240', 'OTC6432', 'OTC4760', 'OTC3691', 'OTC6195', 'OTC4803', 'OTC6279', 'OTC3623', 'OTC3541', 'OTC6508', 'OTC3444', 'OTC6146', 'OTC2743', 'OTC8423', 'OTC3499', 'OTC4127', 'OTC5206', 'OTC8068', 'OTC5392', 'OTC5202', 'OTC5483', 'OTC6242', 'OTC3710', 'OTC2916', 'OTC6173', 'OTC6228', 'OTC3431', 'OTC8176', 'OTC6223', 'OTC6803', 'OTC8042', 'OTC4523', 'OTC4305', 'OTC6238', 'OTC5398', 'OTC2065', 'OTC8477', 'OTC8284', 'OTC5450', 'OTC8044', 'OTC8096', 'OTC6129', 'OTC5011', 'OTC3114', 'OTC8287', 'OTC4991', 'OTC4728', 'OTC8093', 'OTC3095', 'OTC7402', 'OTC3313', 'OTC4950', 'OTC3624', 'OTC4419', 'OTC5321', 'OTC5432', 'OTC5349', 'OTC6188', 'OTC5013', 'OTC3290', 'OTC3492', 'OTC3615', 'OTC6175', 'OTC6577', 'OTC6697', 'OTC8147', 'OTC2926', 'OTC6292', 'OTC3188', 'OTC3171', 'OTC4530', 'OTC3426', 'OTC3131', 'OTC3531', 'OTC6148', 'OTC2736', 'OTC4198', 'OTC3357', 'OTC2641', 'OTC4153', 'OTC6130', 'OTC3551', 'OTC3484', 'OTC1785', 'OTC3684', 'OTC6588', 'OTC5015', 'OTC4109', 'OTC5245', 'OTC3169', 'OTC6248', 'OTC8089', 'OTC3689', 'OTC5523', 'OTC6461', 'OTC3652', 'OTC5426', 'OTC2928', 'OTC3709', 'OTC6741', 'OTC4909', 'OTC6680', 'OTC4183', 'OTC3086', 'OTC4105', 'OTC4541', 'OTC8426', 'OTC6276', 'OTC8279', 'OTC6208', 'OTC4924', 'OTC3489', 'OTC6103', 'OTC6121', 'OTC6629', 'OTC3152', 'OTC1586', 'OTC3526', 'OTC3224', 'OTC6690', 'OTC8091', 'OTC9960', 'OTC6113', 'OTC6167', 'OTC6207', 'OTC6603', 'OTC4979', 'OTC9962', 'OTC6499', 'OTC4171', 'OTC3293', 'OTC3219', 'OTC1815', 'OTC3540', 'OTC3218', 'OTC5905', 'OTC6593', 'OTC6615', 'OTC6751', 'OTC3434', 'OTC4402', 'OTC4192', 'OTC3455', 'OTC4417', 'OTC3081', 'OTC5516', 'OTC4126', 'OTC1593', 'OTC8937', 'OTC6643', 'OTC8390', 'OTC3285', 'OTC1813', 'OTC3490', 'OTC5278', 'OTC6241', 'OTC5355', 'OTC3297', 'OTC6290', 'OTC2221', 'OTC3272', 'OTC5530', 'OTC5220', 'OTC5328', 'OTC4116', 'OTC6523', 'OTC6870', 'OTC3324', 'OTC5455', 'OTC3306', 'OTC3609', 'OTC3211', 'OTC5230', 'OTC6728', 'OTC8107', 'OTC3322', 'OTC5227', 'OTC4538', 'OTC8942', 'OTC3390', 'OTC3363', 'OTC3372', 'OTC6488', 'OTC4529', 'OTC5348', 'OTC2732', 'OTC6874', 'OTC6548', 'OTC6404', 'OTC3232', 'OTC3276', 'OTC6265', 'OTC5514', 'OTC3118', 'OTC8433', 'OTC4946', 'OTC1599', 'OTC6203', 'OTC8403', 'OTC5508', 'OTC3093', 'OTC4130', 'OTC6266', 'OTC3707', 'OTC6185', 'OTC4971', 'OTC6270', 'OTC3388', 'OTC1259', 'OTC8929', 'OTC4580', 'OTC8109', 'OTC6217', 'OTC6843', 'OTC6123', 'OTC5102', 'OTC5263', 'OTC3236', 'OTC4542', 'OTC8938', 'OTC4933', 'OTC4711', 'OTC5289', 'OTC3234', 'OTC4162', 'OTC8027', 'OTC6114', 'OTC5317', 'OTC3552', 'OTC3163', 'OTC6649', 'OTC6134', 'OTC4944', 'OTC5205', 'OTC2640', 'OTC6118', 'OTC5309', 'OTC5609', 'OTC4528', 'OTC5276', 'OTC4729', 'OTC3466', 'OTC4743', 'OTC9949', 'OTC5310', 'OTC8415', 'OTC3581', 'OTC8906', 'OTC8050', 'OTC3498', 'OTC4561', 'OTC5209', 'OTC5520', 'OTC1264', 'OTC5403', 'OTC4911', 'OTC6174', 'OTC2596', 'OTC6594', 'OTC5251', 'OTC6156', 'OTC2235', 'OTC2070', 'OTC4506', 'OTC6561', 'OTC5210', 'OTC6221', 'OTC6542', 'OTC1799', 'OTC4152', 'OTC8420', 'OTC6264', 'OTC6144', 'OTC3078', 'OTC6840', 'OTC2035', 'OTC4510', 'OTC1268', 'OTC8928', 'OTC8040', 'OTC4804', 'OTC6788', 'OTC6233', 'OTC3202', 'OTC4120', 'OTC4550', 'OTC6291', 'OTC4207', 'OTC4513', 'OTC5302', 'OTC3508', 'OTC4175', 'OTC6284', 'OTC6275', 'OTC6187', 'OTC8038', 'OTC6465', 'OTC6234', 'OTC3071', 'OTC5009', 'OTC6125', 'OTC5272', 'OTC4767', 'OTC8354', 'OTC5820', 'OTC6679', 'OTC6259', 'OTC4303', 'OTC1752', 'OTC3205', 'OTC2734', 'OTC4987', 'OTC4745', 'OTC8240', 'OTC6494', 'OTC6186', 'OTC5344', 'OTC4304', 'OTC6150', 'OTC6219', 'OTC3122', 'OTC4180', 'OTC6104', 'OTC4161', 'OTC4167', 'OTC3085', 'OTC6240', 'OTC6761', 'OTC8092', 'OTC3664', 'OTC6512', 'OTC6510', 'OTC2754', 'OTC3546', 'OTC6569', 'OTC3516', 'OTC3555', 'OTC6642', 'OTC6530', 'OTC4433', 'OTC3520', 'OTC4174', 'OTC8358', 'OTC8933', 'OTC6015', 'OTC6158', 'OTC2740', 'OTC5315', 'OTC5474', 'OTC4714', 'OTC6417', 'OTC5274', 'OTC5493', 'OTC3483', 'OTC3206', 'OTC3373', 'OTC4563', 'OTC4556', 'OTC8066', 'OTC4966', 'OTC6122', 'OTC6418', 'OTC4543', 'OTC8086', 'OTC3207', 'OTC3663', 'OTC3672', 'OTC4173', 'OTC8472', 'OTC4945', 'OTC6716', 'OTC6457', 'OTC6274', 'OTC6151', 'OTC6435', 'OTC6140', 'OTC1566', 'OTC8048', 'OTC4947', 'OTC8917', 'OTC6538', 'OTC4168', 'OTC6199', 'OTC4420', 'OTC5211', 'OTC3073', 'OTC5236', 'OTC3067', 'OTC4157', 'OTC5904', 'OTC3332', 'OTC3558', 'OTC3631', 'OTC3147', 'OTC6514', 'OTC5201', 'OTC5498', 'OTC6179', 'OTC6190', 'TSE1436', 'TSE1735', 'TSE1504', 'TSE6531', 'TSE2387', 'TSE3052', 'TSE2332', 'TSE4916', 'TSE2013', 'TSE3046', 'TSE1229', 'TSE4141', 'TSE2430', 'TSE2739', 'TSE3045', 'TSE1227', 'TSE2330', 'TSE1453', 'TSE1201', 'TSE8480', 'TSE6655', 'TSE2534', 'TSE2923', 'TSE6706', 'TSE2459', 'TSE6128', 'TSE6215', 'TSE4306', 'TSE1730', 'TSE1443', 'TSE4746', 'TSE2429', 'TSE6172', 'TSE8021', 'TSE5607', 'TSE2102', 'TSE1806', 'TSE1409', 'TSE2597', 'TSE1307', 'TSE2204', 'TSE4440', 'TSE2888', 'TSE4935', 'TSE2017', 'TSE4919', 'TSE6225', 'TSE2034', 'TSE9935', 'TSE1773', 'TSE2516', 'TSE1537', 'TSE2433', 'TSE9912', 'TSE4915', 'TSE2607', 'TSE1760', 'TSE2337', 'TSE1711', 'TSE1234', 'TSE3528', 'TSE8104', 'TSE3607', 'TSE6257', 'TSE6790', 'TSE9924', 'TSE1471', 'TSE1609', 'TSE3041', 'TSE1402', 'TSE1476', 'TSE1417', 'TSE6671', 'TSE1319', 'TSE3027', 'TSE1718', 'TSE1419', 'TSE4904', 'TSE4552', 'TSE4942', 'TSE6024', 'TSE2024', 'TSE2373', 'TSE6451', 'TSE2374', 'TSE1103', 'TSE2313', 'TSE6282', 'TSE6209', 'TSE4119', 'TSE3702', 'TSE2393', 'TSE2474', 'TSE8442', 'TSE2816', 'TSE2107', 'TSE8996', 'TSE6139', 'TSE3015', 'TSE4148', 'TSE3094', 'TSE3029', 'TSE2250', 'TSE1731', 'TSE2901', 'TSE6715', 'TSE2520', 'TSE4526', 'TSE2397', 'TSE4564', 'TSE5215', 'TSE2014', 'TSE3038', 'TSE3003', 'TSE2206', 'TSE6666', 'TSE2908', 'TSE2440', 'TSE8427', 'TSE1568', 'TSE2617', 'TSE6136', 'TSE4967', 'TSE6289', 'TSE1225', 'TSE6131', 'TSE2897', 'TSE2442', 'TSE3044', 'TSE1539', 'TSE6672', 'TSE1439', 'TSE3308', 'TSE2528', 'TSE1626', 'TSE2748', 'TSE2497', 'TSE5222', 'TSE1432', 'TSE3356', 'TSE2388', 'TSE2423', 'TSE2905', 'TSE1102', 'TSE1437', 'TSE8443', 'TSE2022', 'TSE3533', 'TSE2903', 'TSE3376', 'TSE8429', 'TSE1515', 'TSE6155', 'TSE1473', 'TSE2836', 'TSE1109', 'TSE2912', 'TSE4961', 'TSE2707', 'TSE1903', 'TSE2530', 'TSE3021', 'TSE1303', 'TSE1470', 'TSE3043', 'TSE9939', 'TSE1101', 'TSE2404', 'TSE3591', 'TSE6573', 'TSE6505', 'TSE2436', 'TSE2456', 'TSE6201', 'TSE3311', 'TSE6288', 'TSE3008', 'TSE3338', 'TSE6756', 'TSE6213', 'TSE1218', 'TSE1337', 'TSE1414', 'TSE8940', 'TSE4581', 'TSE1309', 'TSE4952', 'TSE1233', 'TSE5225', 'TSE9931', 'TSE3037', 'TSE2881', 'TSE3002', 'TSE3679', 'TSE5531', 'TSE2801', 'TSE1477', 'TSE2308', 'TSE8163', 'TSE4807', 'TSE2208', 'TSE2324', 'TSE6592', 'TSE1533', 'TSE1321', 'TSE3501', 'TSE2636', 'TSE1460', 'TSE3669', 'TSE6196', 'TSE8114', 'TSE2426', 'TSE3504', 'TSE3058', 'TSE5007', 'TSE1216', 'TSE2356', 'TSE2547', 'TSE2354', 'TSE6442', 'TSE5876', 'TSE1598', 'TSE6591', 'TSE2032', 'TSE1506', 'TSE2069', 'TSE3305', 'TSE8463', 'TSE2227', 'TSE6166', 'TSE4722', 'TSE1215', 'TSE1413', 'TSE2504', 'TSE2482', 'TSE2453', 'TSE3016', 'TSE6226', 'TSE2415', 'TSE9929', 'TSE1457', 'TSE3018', 'TSE6239', 'TSE2891', 'TSE2911', 'TSE6792', 'TSE1308', 'TSE9940', 'TSE4956', 'TSE1315', 'TSE2355', 'TSE4144', 'TSE1512', 'TSE6525', 'TSE6184', 'TSE2059', 'TSE4557', 'TSE2906', 'TSE2537', 'TSE2483', 'TSE1531', 'TSE2832', 'TSE1717', 'TSE6533', 'TSE3010', 'TSE1535', 'TSE2506', 'TSE6189', 'TSE2012', 'TSE2228', 'TSE3536', 'TSE2369', 'TSE1529', 'TSE3701', 'TSE1722', 'TSE6285', 'TSE1463', 'TSE6243', 'TSE1725', 'TSE6477', 'TSE8105', 'TSE2302', 'TSE3051', 'TSE2478', 'TSE2030', 'TSE3682', 'TSE3059', 'TSE2892', 'TSE6152', 'TSE8404', 'TSE1526', 'TSE4725', 'TSE5471', 'TSE2458', 'TSE2357', 'TSE3014', 'TSE9917', 'TSE6183', 'TSE2615', 'TSE5243', 'TSE2451', 'TSE1527', 'TSE3312', 'TSE2405', 'TSE5521', 'TSE2491', 'TSE2618', 'TSE1232', 'TSE2852', 'TSE3543', 'TSE3673', 'TSE6464', 'TSE4108', 'TSE1467', 'TSE1313', 'TSE4912', 'TSE2009', 'TSE5264', 'TSE3708', 'TSE8046', 'TSE2485', 'TSE3494', 'TSE9946', 'TSE2543', 'TSE3017', 'TSE1451', 'TSE2492', 'TSE6581', 'TSE3189', 'TSE2029', 'TSE3030', 'TSE2489', 'TSE1726', 'TSE3518', 'TSE3694', 'TSE1340', 'TSE5269', 'TSE4106', 'TSE2101', 'TSE3031', 'TSE2936', 'TSE9958', 'TSE4164', 'TSE1616', 'TSE3032', 'TSE3062', 'TSE2342', 'TSE1236', 'TSE2514', 'TSE6806', 'TSE9943', 'TSE1597', 'TSE3419', 'TSE6115', 'TSE6552', 'TSE2049', 'TSE1582', 'TSE3383', 'TSE2360', 'TSE4133', 'TSE1440', 'TSE2481', 'TSE2371', 'TSE2243', 'TSE9911', 'TSE2493', 'TSE4943', 'TSE2705', 'TSE3024', 'TSE4545', 'TSE1338', 'TSE3596', 'TSE1326', 'TSE8422', 'TSE1604', 'TSE2727', 'TSE1538', 'TSE2882', 'TSE1213', 'TSE2323', 'TSE2424', 'TSE2367', 'TSE9914', 'TSE1262', 'TSE2834', 'TSE3588', 'TSE2886', 'TSE4562', 'TSE3049', 'TSE2611', 'TSE5706', 'TSE3557', 'TSE2331', 'TSE1454', 'TSE5284', 'TSE6283', 'TSE1809', 'TSE2515', 'TSE1536', 'TSE2419', 'TSE2910', 'TSE1468', 'TSE3006', 'TSE2753', 'TSE4572', 'TSE8011', 'TSE1441', 'TSE6281', 'TSE1220', 'TSE2450', 'TSE6605', 'TSE1808', 'TSE1305', 'TSE2103', 'TSE3545', 'TSE4737', 'TSE2104', 'TSE2028', 'TSE1720', 'TSE5538', 'TSE6120', 'TSE3130', 'TSE1909', 'TSE9921', 'TSE4764', 'TSE6005', 'TSE1905', 'TSE6668', 'TSE3167', 'TSE1410', 'TSE3013', 'TSE1733', 'TSE2207', 'TSE1517', 'TSE2395', 'TSE1902', 'TSE9933', 'TSE5871', 'TSE2540', 'TSE1449', 'TSE3149', 'TSE1702', 'TSE2633', 'TSE1605', 'TSE5434', 'TSE2359', 'TSE3665', 'TSE8482', 'TSE1704', 'TSE2105', 'TSE2612', 'TSE4938', 'TSE9934', 'TSE3432', 'TSE6192', 'TSE2114', 'TSE2364', 'TSE2883', 'TSE2425', 'TSE1444', 'TSE2885', 'TSE1312', 'TSE2438', 'TSE6133', 'TSE8478', 'TSE2476', 'TSE1507', 'TSE2484', 'TSE2239', 'TSE2305', 'TSE2723', 'TSE8103', 'TSE2365', 'TSE6271', 'TSE4438', 'TSE3653', 'TSE1256', 'TSE1472', 'TSE4155', 'TSE2444', 'TSE6515', 'TSE1465', 'TSE8488', 'TSE6197', 'TSE2460', 'TSE1314', 'TSE2385', 'TSE9927', 'TSE6230', 'TSE2468', 'TSE1466', 'TSE8222', 'TSE2850', 'TSE6176', 'TSE2889', 'TSE3406', 'TSE2376', 'TSE1615', 'TSE6770', 'TSE1721', 'TSE3563', 'TSE9906', 'TSE1210', 'TSE6625', 'TSE3705', 'TSE2338', 'TSE1701', 'TSE2399', 'TSE9908', 'TSE8462', 'TSE6781', 'TSE3712', 'TSE3034', 'TSE2608', 'TSE2465', 'TSE3346', 'TSE2609', 'TSE2347', 'TSE6165', 'TSE2408', 'TSE8926', 'TSE6598', 'TSE3056', 'TSE2913', 'TSE8464', 'TSE3266', 'TSE8374', 'TSE1614', 'TSE2015', 'TSE3532', 'TSE3593', 'TSE4190', 'TSE3515', 'TSE2382', 'TSE2201', 'TSE8016', 'TSE6426', 'TSE5906', 'TSE9907', 'TSE8039', 'TSE3443', 'TSE1341', 'TSE2062', 'TSE1456', 'TSE3661', 'TSE4560', 'TSE1416', 'TSE6168', 'TSE1231', 'TSE5484', 'TSE2884', 'TSE3042', 'TSE1789', 'TSE5259', 'TSE2108', 'TSE3454', 'TSE1734', 'TSE2020', 'TSE2505', 'TSE6108', 'TSE1795', 'TSE1434', 'TSE2362', 'TSE3209', 'TSE6191', 'TSE6669', 'TSE1810', 'TSE2316', 'TSE1446', 'TSE6438', 'TSE2420', 'TSE3321', 'TSE2033', 'TSE4989', 'TSE2630', 'TSE2401', 'TSE4960', 'TSE2464', 'TSE1475', 'TSE3437', 'TSE1708', 'TSE2603', 'TSE4968', 'TSE3019', 'TSE9919', 'TSE4755', 'TSE9941', 'TSE4571', 'TSE3047', 'TSE2462', 'TSE2008', 'TSE6754', 'TSE2605', 'TSE6414', 'TSE2441', 'TSE1524', 'TSE2402', 'TSE2511', 'TSE1525', 'TSE2303', 'TSE9802', 'TSE4536', 'TSE2312', 'TSE1540', 'TSE5519', 'TSE8249', 'TSE6431', 'TSE3605', 'TSE6205', 'TSE1560', 'TSE6216', 'TSE2457', 'TSE6491', 'TSE2380', 'TSE2722', 'TSE2344', 'TSE1714', 'TSE2377', 'TSE3583', 'TSE2477', 'TSE5608', 'TSE3229', 'TSE1514', 'TSE8271', 'TSE3036', 'TSE2328', 'TSE6768', 'TSE2867', 'TSE6416', 'TSE9955', 'TSE3025', 'TSE1907', 'TSE6269', 'TSE8112', 'TSE3550', 'TSE3033', 'TSE2406', 'TSE2348', 'TSE6743', 'TSE2823', 'TSE8072', 'TSE5283', 'TSE1235', 'TSE3231', 'TSE4720', 'TSE1608', 'TSE8367', 'TSE2545', 'TSE6277', 'TSE3138', 'TSE2887', 'TSE2301', 'TSE3048', 'TSE2427', 'TSE1519', 'TSE2904', 'TSE1342', 'TSE1474', 'TSE3011', 'TSE2038', 'TSE4739', 'TSE5469', 'TSE9904', 'TSE1713', 'TSE1528', 'TSE4763', 'TSE2340', 'TSE2409', 'TSE4766', 'TSE3257', 'TSE2527', 'TSE2712', 'TSE9942', 'TSE2449', 'TSE1617', 'TSE1445', 'TSE9937', 'TSE2496', 'TSE2383', 'TSE8131', 'TSE2466', 'TSE6541', 'TSE2548', 'TSE2025', 'TSE1452', 'TSE2634', 'TSE2379', 'TSE2915', 'TSE2467', 'TSE3413', 'TSE3535', 'TSE2321', 'TSE2349', 'TSE6112', 'TSE6214', 'TSE6670', 'TSE4977', 'TSE2538', 'TSE3416', 'TSE1530', 'TSE2812', 'TSE1503', 'TSE4566', 'TSE4576', 'TSE3090', 'TSE1110', 'TSE2809', 'TSE2704', 'TSE5522', 'TSE8215', 'TSE8081', 'TSE8210', 'TSE2241', 'TSE3035', 'TSE1723', 'TSE8213', 'TSE1310', 'TSE1590', 'TSE1904', 'TSE1339', 'TSE4934', 'TSE9944', 'TSE2845', 'TSE1301', 'TSE2023', 'TSE2417', 'TSE6641', 'TSE2637', 'TSE2390', 'TSE2546', 'TSE5305', 'TSE1587', 'TSE2498', 'TSE2421', 'TSE1592', 'TSE2841', 'TSE2454', 'TSE1219', 'TSE2851', 'TSE8261', 'TSE4532', 'TSE3022', 'TSE3576', 'TSE4439', 'TSE2536', 'TSE3057', 'TSE3005', 'TSE3060', 'TSE4137', 'TSE4999', 'TSE9938', 'TSE8201', 'TSE2327', 'TSE9930', 'TSE2345', 'TSE2890', 'TSE4770', 'TSE1217', 'TSE3450', 'TSE1464', 'TSE2358', 'TSE3164', 'TSE6224', 'TSE1541', 'TSE3092', 'TSE2363', 'TSE2211', 'TSE1736', 'TSE8070', 'TSE6117', 'TSE3645', 'TSE6449', 'TSE2616', 'TSE4551', 'TSE8467', 'TSE6443', 'TSE2455', 'TSE1724', 'TSE6456', 'TSE6776', 'TSE8341', 'TSE5203', 'TSE2236', 'TSE2031', 'TSE5546', 'TSE2431', 'TSE4906', 'TSE4930', 'TSE5285', 'TSE1324', 'TSE9902', 'TSE1583', 'TSE8110', 'TSE1603', 'TSE3703', 'TSE1104', 'TSE4958', 'TSE4976', 'TSE1447', 'TSE9905', 'TSE8028', 'TSE4104', 'TSE1316', 'TSE8473', 'TSE5515', 'TSE6409', 'TSE6116', 'TSE2731', 'TSE2329', 'TSE2701', 'TSE3714', 'TSE2706', 'TSE6235', 'TSE3704', 'TSE4927', 'TSE3711', 'TSE1423', 'TSE8101', 'TSE1203', 'TSE3028', 'TSE9928', 'TSE2509', 'TSE2535', 'TSE5258', 'TSE2610', 'TSE1455', 'TSE3698', 'TSE1817', 'TSE3050', 'TSE2428', 'TSE5880', 'TSE1802', 'TSE1558', 'TSE4414', 'TSE1108', 'TSE6412', 'TSE8033', 'TSE2434', 'TSE2849', 'TSE2838', 'TSE6579', 'TSE2007', 'TSE5534', 'TSE2614', 'TSE1737', 'TSE1786', 'TSE6142', 'TSE2314', 'TSE1516', 'TSE2472', 'TSE2945', 'TSE3055', 'TSE1612', 'TSE2486', 'TSE6278', 'TSE6164', 'TSE8481', 'TSE2495', 'TSE8454', 'TSE2939', 'TSE1522', 'TSE1459', 'TSE4142', 'TSE8466', 'TSE2351', 'TSE3686', 'TSE6206', 'TSE3004', 'TSE1435', 'TSE2501', 'TSE2392', 'TSE1732', 'TSE4540', 'TSE2820', 'TSE2601', 'TSE2368', 'TSE9926', 'TSE3380', 'TSE2499', 'TSE5533', 'TSE8497', 'TSE1589', 'TSE5907', 'TSE4426', 'TSE1418', 'TSE8411', 'TSE2231', 'TSE2233', 'TSE9918', 'TSE2413', 'TSE1521', 'TSE5525', 'TSE2471', 'TSE1776', 'TSE8150', 'TSE2439', 'TSE2461', 'TSE6405', 'TSE2353', 'TSE2702', 'TSE9945', 'TSE1709', 'TSE2010', 'TSE2412', 'TSE2855', 'TSE6177', 'TSE4555', 'TSE9910', 'TSE2613', 'TSE2414', 'TSE1805', 'TSE1513', 'TSE1323', 'TSE3054', 'TSE6415', 'TSE1438', 'TSE4994', 'TSE5388', 'TSE6674', 'TSE2006', 'TSE1442', 'TSE3706', 'TSE2247', 'TSE1906', 'TSE2488', 'TSE2539', 'TSE6558', 'TSE6202', 'TSE2027', 'TSE5288', 'TSE3296', 'TSE3530', 'TSE2880', 'TSE1783', 'TSE2929', 'TSE2443', 'TSE1304', 'TSE1532', 'TSE2375', 'TSE6141', 'TSE6251', 'TSE3040', 'TSE6452', 'TSE1618', 'TSE6698', 'TSE3622', 'TSE1611', 'TSE3617', 'TSE3026', 'TSE5234', 'TSE8499', 'TSE1727', 'TSE3481', 'TSE2352', 'TSE1762', 'TSE2109', 'TSE2106', 'TSE6582', 'TSE2524', 'TSE6504', 'TSE9925', 'TSE6153', 'TSE1325', 'TSE2317', 'TSE2448', 'TSE2642', 'TSE2606', 'TSE2002', 'TSE3023', 'TSE1707', 'TSE1712', 'TSE2542', 'TSE2480', 'TSE2115', 'TSE1710'] \n",
            " 總共有1786項\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diPAgsyxXph0"
      },
      "source": [
        "#捨不得刪"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvAq7F5vTZkv"
      },
      "outputs": [],
      "source": [
        "#n= 0\n",
        "#for code in stocks_table.code:\n",
        "#  n += 1\n",
        "#  kbars = api.kbars(api.Contracts.Stocks[f'{code}'], start=start)\n",
        "#  df = pd.DataFrame({**kbars}, index= pd.to_datetime(kbars.ts)).drop('ts', axis= 1)\n",
        "#  df_5min = df.resample(rule='5Min', closed='right').agg({\n",
        "#        'Open': 'first', \n",
        "#        'High': 'max', \n",
        "#        'Low': 'min', \n",
        "#        'Close': 'last',\n",
        "#        'Volume': 'sum'})\n",
        "#  df_day = df.resample(rule='D').agg({\n",
        "#        'Open': 'first', \n",
        "#        'High': 'max', \n",
        "#        'Low': 'min', \n",
        "#        'Close': 'last',\n",
        "#        'Volume': 'sum'})\n",
        "#  df_week = df.resample(rule='W').agg({\n",
        "#        'Open': 'first', \n",
        "#        'High': 'max', \n",
        "#        'Low': 'min', \n",
        "#        'Close': 'last',\n",
        "#        'Volume': 'sum'})\n",
        "#  close1_5min = df_5min.Close.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  close1_day = df_day.Close.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  close1_week = df_week.Close.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#\n",
        "#  open1_5min = df_5min.Open.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  open1_day = df_day.Open.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  open1_week = df_week.Open.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#\n",
        "#  high1_5min = df_5min.High.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  high1_day = df_day.High.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  high1_week = df_week.High.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#\n",
        "#  low1_5min = df_5min.Low.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  low1_day = df_day.Low.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  low1_week = df_week.Low.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#\n",
        "#  volume1_5min = df_5min.Volume.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  volume1_day = df_day.Volume.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  volume1_week = df_week.Volume.rename(stocks_table[stocks_table.code == code].name.to_list()[0])\n",
        "#  try:\n",
        "#    close2_5min = pd.concat([close2_5min, close1_5min], axis=1)\n",
        "#    close2_day = pd.concat([close2_day, close1_day], axis=1)\n",
        "#    close2_week = pd.concat([close2_week, close1_week], axis=1)\n",
        "#\n",
        "#    open2_5min = pd.concat([open2_5min, open1_5min], axis=1)\n",
        "#    open2_day = pd.concat([open2_day, open1_day], axis=1)\n",
        "#    open2_week = pd.concat([open2_week, open1_week], axis=1)\n",
        "#\n",
        "#    high2_5min = pd.concat([high2_5min, high1_5min], axis=1)\n",
        "#    high2_day = pd.concat([high2_day, high1_day], axis=1)\n",
        "#    high2_week = pd.concat([high2_week, high1_week], axis=1)\n",
        "#\n",
        "#    low2_5min = pd.concat([low2_5min, low1_5min], axis=1)\n",
        "#    low2_day = pd.concat([low2_day, low1_day], axis=1)\n",
        "#    low2_week = pd.concat([low2_week, low1_week], axis=1)\n",
        "#\n",
        "#    volume2_5min = pd.concat([volume2_5min, volume1_5min], axis=1)\n",
        "#    volume2_day = pd.concat([volume2_day, volume1_day], axis=1)\n",
        "#    volume2_week = pd.concat([volume2_week, volume1_week], axis=1)\n",
        "#  except:\n",
        "#    close2_5min = close1_5min\n",
        "#    close2_day = close1_day\n",
        "#    close2_week = close1_week\n",
        "#\n",
        "#    open2_5min = open1_5min\n",
        "#    open2_day = open1_day\n",
        "#    open2_week = open1_week\n",
        "#\n",
        "#    high2_5min = high1_5min\n",
        "#    high2_day = high1_day\n",
        "#    high2_week = high1_week\n",
        "#\n",
        "#    low2_5min = low1_5min\n",
        "#    low2_day = low1_day\n",
        "#    low2_week = low1_week\n",
        "#\n",
        "#    volume2_5min = volume1_5min\n",
        "#    volume2_day = volume1_day\n",
        "#    volume2_week = volume1_week\n",
        "#  print('還有{}項'.format(len(stocks_table.code) - n))\n",
        "#\n",
        "#\n",
        "##close\n",
        "#close2_5min.fillna(0, inplace= True)\n",
        "#close2_5min.sort_index()\n",
        "#close2_5min.reset_index(inplace= True)\n",
        "#close2_5min.to_gbq('close_5min', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#close2_day.fillna(0, inplace= True)\n",
        "#close2_day.sort_index()\n",
        "#close2_day.reset_index(inplace= True)\n",
        "#close2_day.to_gbq('close_day', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#close2_week.fillna(0, inplace= True)\n",
        "#close2_week.sort_index()\n",
        "#close2_week.reset_index(inplace= True)\n",
        "#close2_week.to_gbq('close_week', project_id= project_id, if_exists='append')\n",
        "#\n",
        "##open\n",
        "#open2_5min.fillna(0, inplace= True)\n",
        "#open2_5min.sort_index()\n",
        "#open2_5min.reset_index(inplace= True)\n",
        "#open2_5min.to_gbq('open_5min', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#open2_day.fillna(0, inplace= True)\n",
        "#open2_day.sort_index()\n",
        "#open2_day.reset_index(inplace= True)\n",
        "#open2_day.to_gbq('open_day', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#open2_week.fillna(0, inplace= True)\n",
        "#open2_week.sort_index()\n",
        "#open2_week.reset_index(inplace= True)\n",
        "#open2_week.to_gbq('open_week', project_id= project_id, if_exists='append')\n",
        "#\n",
        "##high\n",
        "#high2_5min.fillna(0, inplace= True)\n",
        "#high2_5min.sort_index()\n",
        "#high2_5min.reset_index(inplace= True)\n",
        "#high2_5min.to_gbq('high_5min', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#high2_day.fillna(0, inplace= True)\n",
        "#high2_day.sort_index()\n",
        "#high2_day.reset_index(inplace= True)\n",
        "#high2_day.to_gbq('high_day', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#high2_week.fillna(0, inplace= True)\n",
        "#high2_week.sort_index()\n",
        "#high2_week.reset_index(inplace= True)\n",
        "#high2_week.to_gbq('high_week', project_id= project_id, if_exists='append')\n",
        "#\n",
        "##low\n",
        "#low2_5min.fillna(0, inplace= True)\n",
        "#low2_5min.sort_index()\n",
        "#low2_5min.reset_index(inplace= True)\n",
        "#low2_5min.to_gbq('low_5min', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#low2_day.fillna(0, inplace= True)\n",
        "#low2_day.sort_index()\n",
        "#low2_day.reset_index(inplace= True)\n",
        "#low2_day.to_gbq('low_day', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#low2_week.fillna(0, inplace= True)\n",
        "#low2_week.sort_index()\n",
        "#low2_week.reset_index(inplace= True)\n",
        "#low2_week.to_gbq('low_week', project_id= project_id, if_exists='append')\n",
        "#\n",
        "##volume\n",
        "#volume2_5min.fillna(0, inplace= True)\n",
        "#volume2_5min.sort_index()\n",
        "#volume2_5min.reset_index(inplace= True)\n",
        "#volume2_5min.to_gbq('volume_5min', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#volume2_day.fillna(0, inplace= True)\n",
        "#volume2_day.sort_index()\n",
        "#volume2_day.reset_index(inplace= True)\n",
        "#volume2_day.to_gbq('volume_day', project_id= project_id, if_exists='append')\n",
        "#\n",
        "#volume2_week.fillna(0, inplace= True)\n",
        "#volume2_week.sort_index()\n",
        "#volume2_week.reset_index(inplace= True)\n",
        "#volume2_week.to_gbq('volume_week', project_id= project_id, if_exists='append')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgc9eYlD55EL"
      },
      "outputs": [],
      "source": [
        "#for code in list_code:\n",
        "#\n",
        "#  code_5min = code\n",
        "#  code_day = code\n",
        "#  code_week = code\n",
        "#\n",
        "#  if code_5min in set(existing_kbar_5min.Code):\n",
        "#    number_5min = ((pd.Series(list_code)[pd.Series(list_code) == code_5min]).index[0]) + 1\n",
        "#    if pd.to_datetime(existing_kbar_5min[existing_kbar_5min.Code == code_5min].last_update.to_list()) < LAST_TRADE_DATE:\n",
        "#      kbar_5min(code_5min, start = ([*existing_kbar_5min[existing_kbar_5min.Code == code_5min].last_update][0] + timedelta(days= 1)).strftime('%Y-%m-%d'))\n",
        "#      print('[已有資料]還剩下{:,}項'.format((len(list_code) - number_5min)))\n",
        "#    else:\n",
        "#      pass\n",
        "#  elif code_5min not in set(existing_kbar_5min.Code):\n",
        "#    kbar_5min(code_5min)\n",
        "#    number_5min = ((pd.Series(list_code)[pd.Series(list_code) == code]).index[0]) + 1\n",
        "#    print('還剩下{:,}項'.format((len(list_code) - number_5min)))\n",
        "#\n",
        "#  if code_day in set(existing_kbar_day.Code):\n",
        "#    number_day = ((pd.Series(list_code)[pd.Series(list_code) == code_day]).index[0]) + 1\n",
        "#    if pd.to_datetime(existing_kbar_day[existing_kbar_day.Code == code_day].last_update.to_list()) < LAST_TRADE_DATE:\n",
        "#      kbar_day(code_day, start = ([*existing_kbar_day[existing_kbar_day.Code == code_day].last_update][0] + timedelta(days= 1)).strftime('%Y-%m-%d'))\n",
        "#      print('[已有資料]還剩下{:,}項'.format(len(list_code) - len(existing_kbar_day.Code) - number_day))\n",
        "#    else:\n",
        "#      pass\n",
        "#  elif code_day not in set(existing_kbar_day.Code):\n",
        "#    number_day = ((pd.Series(list_code)[pd.Series(list_code) == code_day]).index[0]) + 1\n",
        "#    kbar_day(code_day)\n",
        "#    print('還剩下{:,}項'.format(len(list_code) - len(existing_kbar_day) - number_day))\n",
        "#  \n",
        "#  if code_week in set(existing_kbar_week.Code):\n",
        "#    number_week = ((pd.Series(list_code)[pd.Series(list_code) == code_week]).index[0]) + 1\n",
        "#    if pd.to_datetime(existing_kbar_week[existing_kbar_week.Code == code_week].last_update.to_list()) < LAST_TRADE_DATE:\n",
        "#      kbar_week(code_week, start = ([*existing_kbar_week[existing_kbar_week.Code == code_week].last_update][0] + timedelta(days= 1)).strftime('%Y-%m-%d'))\n",
        "#      print('[已有資料]還剩下{:,}項'.format(len(list_code) - len(existing_kbar_week.Code)- number_week))\n",
        "#    else:\n",
        "#      pass\n",
        "#  elif code_week not in set(existing_kbar_week.Code):\n",
        "#    number_week = ((pd.Series(list_code)[pd.Series(list_code) == code_week]).index[0]) + 1\n",
        "#    kbar_week(code_week)\n",
        "#    print('還剩下{:,}項'.format(len(list_code) - len(existing_kbar_week) - number_week))\n",
        "#\n",
        "#print('Done')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Q8_dkoCbmsgZ",
        "MALFmvD0apCB",
        "PmHUEp3fcMKa",
        "d38sjmNgXx8m",
        "DATmcmDCk3IS",
        "vH_2FqqLRcf-",
        "upn58UaSRKi0",
        "_4YYzfMjkvFc",
        "PgueYH0wkxae",
        "GUIv-kaGvb4b",
        "J4esaaQUell4",
        "xI-h1UqVd3d5",
        "diPAgsyxXph0"
      ],
      "name": "TRivots-U/L.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}